{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2525fb2f",
   "metadata": {},
   "source": [
    "<div style=\"align: center; margin: 0; padding: 0; height: 250px;\">\n",
    "    <br>\n",
    "    <img src=\"https://images.ctfassets.net/p6ae3zqfb1e3/3pQAQO2G3wrOcyuCXFbAJd/01f1309dda5327d03a76a051f98f44ac/Citi_Bike_Homepage_Hero_3x__1_.jpg\" style=\"display:block; margin:auto; width:65%; height:100%;\">\n",
    "</div><br><br>\n",
    "\n",
    "<div style=\"letter-spacing:normal; opacity:1.;\">\n",
    "<!--   https://xkcd.com/color/rgb/   -->\n",
    "  <p style=\"text-align:center; background-color: lightsalmon; color: Jaguar; border-radius:10px; font-family:monospace; \n",
    "            line-height:1.4; font-size:32px; font-weight:bold; text-transform: uppercase; padding: 9px;\">\n",
    "            <strong>Citi Bike Trip History Data</strong></p>  \n",
    "  \n",
    "  <p style=\"text-align:center; background-color:romance; color: Jaguar; border-radius:10px; font-family:monospace; \n",
    "            line-height:1.4; font-size:22px; font-weight:normal; text-transform: capitalize; padding: 5px;\"\n",
    "     >Machine Learning Module: Ride Duration Prediction using Regression Analysis<br>(EDA and Linear Regression Model Training)</p>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a8b00d",
   "metadata": {},
   "source": [
    "**Dataset Info**\n",
    "\n",
    "> https://citibikenyc.com/system-data\n",
    "\n",
    "**System Data**\n",
    "\n",
    "Where do Citi Bikers ride? When do they ride? How far do they go? Which stations are most popular? What days of the week are most rides taken on? We've heard all of these questions and more from you, and we're happy to provide the data to help you discover the answers to these questions and more. We invite developers, engineers, statisticians, artists, academics and other interested members of the public to use the data we provide for analysis, development, visualization and whatever else moves you.\n",
    "\n",
    "This data is provided according to the [NYCBS Data Use Policy](https://www.citibikenyc.com/data-sharing-policy).\n",
    "\n",
    "\n",
    "**Citi Bike Trip Histories**\n",
    "\n",
    "We publish [downloadable files of Citi Bike trip data](https://s3.amazonaws.com/tripdata/index.html). The data includes:\n",
    "\n",
    "- Ride ID\n",
    "- Rideable type\n",
    "- Started at\n",
    "- Ended at\n",
    "- Start station name\n",
    "- Start station ID\n",
    "- End station name\n",
    "- End station ID\n",
    "- Start latitude\n",
    "- Start longitude\n",
    "- End latitude\n",
    "- End Longitude\n",
    "- Member or casual ride\n",
    "\n",
    "Data format previously:\n",
    "\n",
    "- Trip Duration (seconds)\n",
    "- Start Time and Date\n",
    "- Stop Time and Date\n",
    "- Start Station Name\n",
    "- End Station Name\n",
    "- Station ID\n",
    "- Station Lat/Long\n",
    "- Bike ID\n",
    "- User Type (Customer = 24-hour pass or 3-day pass user; Subscriber = Annual Member)\n",
    "- Gender (Zero=unknown; 1=male; 2=female)\n",
    "- Year of Birth\n",
    "\n",
    "This data has been processed to remove trips that are taken by staff as they service and inspect the system, trips that are taken to/from any of our “test” stations (which we were using more in June and July 2013), and any trips that were below 60 seconds in length (potentially false starts or users trying to re-dock a bike to ensure it's secure).\n",
    "\n",
    "[Download Citi Bike trip history data](https://s3.amazonaws.com/tripdata/index.html)\n",
    "\n",
    "**Monthly Operating Reports**\n",
    "\n",
    "View the [monthly operating reports](https://www.citibikenyc.com/system-data/operating-reports) that we provide to the NYC Department of Transportation.\n",
    "\n",
    "**Additional Resources**\n",
    "\n",
    "- The City of New York's [bicycling data](http://www.nyc.gov/html/dot/html/about/datafeeds.shtml#Bikes)\n",
    "- A group of software developers and data explorers working with data feeds from NYC's Bike Share system and other bike data maintain this [Google Group](https://groups.google.com/forum/#!aboutgroup/citibike-hackers) (note: Citi Bike is not responsible for this group – it is run and maintained by a group of interested private citizens)\n",
    "\n",
    "**TASK**\n",
    "\n",
    "The goal of this project is to apply everything we learned in this course and build an end-to-end machine learning project.\n",
    "\n",
    "Remember that to pass the project, you must evaluate 3 peers. If you don't do that, your project can't be considered compelete.\n",
    "\n",
    "Submitting\n",
    "\n",
    "Project Cohort #2\n",
    "\n",
    "Project:\n",
    "\n",
    "- Form: https://forms.gle/o1s3NmYE4UmFSMVD7\n",
    "- Deadline: 21 August (Monday), 23:00 CEST\n",
    "\n",
    "Peer reviewing:\n",
    "\n",
    "- Peer review assignments: TBA (\"project 2\" tab)\n",
    "- Form: TBA\n",
    "-  Deadline: TBA\n",
    "\n",
    "Project feedback: TBA (\"feedback-02\" tab)\n",
    "\n",
    "Evaluation criteria\n",
    "See [here](https://github.com/DataTalksClub/mlops-zoomcamp/blob/main/07-project/README.md)\n",
    "\n",
    "Questions: https://github.com/DataTalksClub/mlops-zoomcamp/tree/main/cohorts/2023/07-project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78138c11",
   "metadata": {},
   "source": [
    "<div style=\"letter-spacing:normal; opacity:1.;\">\n",
    "  <h1 style=\"text-align:center; background-color: lightsalmon; color: Jaguar; border-radius:10px; font-family:monospace; border-radius:20px;\n",
    "            line-height:1.4; font-size:32px; font-weight:bold; text-transform: uppercase; padding: 9px;\">\n",
    "            <strong>1. Import Libraries & Ingest Data</strong></h1>   \n",
    "</div>\n",
    "\n",
    "> ⚠️ Not Recommended conda `base` env, work on `venv`\n",
    "\n",
    "- https://docs.conda.io/projects/conda/en/4.6.0/_downloads/52a95608c49671267e40c689e0bc00ca/conda-cheatsheet.pdf\n",
    "\n",
    "```\n",
    "pip freeze > requirements.txt\n",
    "conda list -e > requirements.txt\n",
    "\n",
    "# new conda virtual environment\n",
    "conda create --name \"lin-reg\" python=3.10 jupyter -y\n",
    "conda activate \"lin-reg\"\n",
    "\n",
    "# install all package dependencies\n",
    "pip install -r requirements.txt\n",
    "conda install -c conda-forge --file=requirements.txt      # mostly not work\n",
    "conda install -c conda-forge pandas==2.0.2 -q -y\n",
    "\n",
    "# if The environment is inconsistent, try below\n",
    "conda update -n base -c defaults conda --force-reinstall\n",
    "conda install anaconda --force-reinstall\n",
    "\n",
    "```\n",
    "\n",
    "**You must use the `--no-deps` option in the pip install command in order to avoid bundling dependencies into your conda-package.**\n",
    "\n",
    "If you run pip install without the `--no-deps` option, pip will often install dependencies in your conda recipe and those dependencies will become part of your package. This wastes space in the package and `increases the risk of file overlap`, file clobbering, and broken packages.\n",
    "\n",
    "There might be cases where you want to install a package directly from a local directory or a specific location, without relying on the package indexes. In such situations, you can use the `--no-index` option to tell pip not to look for the package in any indexes.\n",
    "\n",
    "```\n",
    "- command1 & command2  # runs simultaneously\n",
    "- command1 ; command2  # runs sequentially\n",
    "- command1 && command2 # runs sequentially, runs command2 only if command1 succeeds\n",
    "- command1 || command2 # runs sequentially, runs command2 only if command1 fails\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81ffeeb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy\n",
    "from scipy.sparse import hstack\n",
    "from scipy.stats import stats\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "# from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "# from sklearn.ensemble import GradientBoostingRegressor, ExtraTreesRegressor\n",
    "# from sklearn.svm import LinearSVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import zipfile\n",
    "# import boto3\n",
    "# import click\n",
    "# import pathlib\n",
    "# import argparse\n",
    "# import requests\n",
    "# import urllib.request\n",
    "from glob import glob\n",
    "# from tqdm import tqdm           # console-based\n",
    "# from tqdm.notebook import tqdm  # jupyter-based\n",
    "from tqdm.auto import tqdm        # automatically selects\n",
    "# tqdm._instances.clear()\n",
    "\n",
    "import mlflow\n",
    "import wandb\n",
    "\n",
    "# memory management performs garbage collection \n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d870c290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlflow.__version__:  2.5.0\n",
      "wandb.__version__ :  0.15.8\n"
     ]
    }
   ],
   "source": [
    "# from mlflow.tracking import MlflowClient\n",
    "# from mlflow.exceptions import MlflowException\n",
    "import mlflow; print(\"mlflow.__version__: \", mlflow.__version__)\n",
    "import wandb;  print(\"wandb.__version__ : \", wandb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e78613e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Get the current working directory\n",
    "# current_dir = os.getcwd()\n",
    "\n",
    "# Create a new directory for storing data\n",
    "os.makedirs('./pycode', exist_ok=True)\n",
    "# os.makedirs('./data', exist_ok=True)\n",
    "os.makedirs('./output', exist_ok=True)\n",
    "os.makedirs('./model', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "044ce591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data\\\\JC-202305-citibike-tripdata.csv',\n",
       " 'data\\\\JC-202305-citibike-tripdata.csv.zip',\n",
       " 'data\\\\JC-202306-citibike-tripdata.csv',\n",
       " 'data\\\\JC-202306-citibike-tripdata.csv.zip',\n",
       " 'data\\\\JC-202307-citibike-tripdata.csv',\n",
       " 'data\\\\JC-202307-citibike-tripdata.csv.zip']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob('data/*.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcc106e",
   "metadata": {},
   "source": [
    "## preprocess_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d48f23bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pycode/preprocess_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pycode/preprocess_data.py\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import zipfile\n",
    "import requests\n",
    "# import pathlib\n",
    "# import urllib.request\n",
    "from glob import glob\n",
    "# from datetime import date, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "def fetch_data(raw_data_path: str, location: str, year: int, month: int) -> None:\n",
    "    \"\"\"Fetches data from the NYC Taxi dataset and saves it locally\"\"\"\n",
    "    filename = f'{location}{year}{month:0>2}-citibike-tripdata.csv.zip'\n",
    "    filepath = os.path.join(raw_data_path, filename)\n",
    "    url      = f'https://s3.amazonaws.com/tripdata/{filename}'\n",
    "\n",
    "    # Create dest_path folder unless it already exists\n",
    "    os.makedirs(raw_data_path, exist_ok=True)\n",
    "    \n",
    "    # Download the data from the NYC Taxi dataset\n",
    "    # os.system(f\"wget -q -N -P {raw_data_path} {url}\")\n",
    "    # urllib.request.urlretrieve(url, filename)\n",
    "    response = requests.get(url)\n",
    "    with open(filepath, \"wb\") as f_out:\n",
    "        f_out.write(response.content)\n",
    "    \n",
    "    # Extract the CSV file from the ZIP file\n",
    "    with zipfile.ZipFile(filepath, 'r') as zip_ref:\n",
    "        zip_ref.extract(filename.replace('.zip', ''), path=raw_data_path)  # Extract to a specific directory\n",
    "    return None\n",
    "\n",
    "\n",
    "def download_data(raw_data_path: str, locations: list, years: list, months: list) -> None:\n",
    "    try:\n",
    "        # Download the data from the NYC Taxi dataset    \n",
    "        for loc in locations: \n",
    "            for year in years:       \n",
    "                for month in months:\n",
    "                    fetch_data(raw_data_path, loc, year, month)\n",
    "    except Exception as e:\n",
    "        print(\"In download_data Something Wrong...\", e)\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def read_data(filename: str) -> pd.DataFrame:\n",
    "    \"\"\"Read data into DataFrame\"\"\"\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    # Convert Datetime\n",
    "    df['started_at'] = pd.to_datetime(df['started_at'])\n",
    "    df['ended_at']   = pd.to_datetime(df['ended_at'])\n",
    "\n",
    "    # Calculate duration, Convert duration to minutes\n",
    "    df['duration'] = df['ended_at'] - df['started_at']\n",
    "    df['duration_minutes'] = df['duration'].dt.total_seconds() / 60\n",
    "\n",
    "    # Define criteria for outliers \n",
    "    lower_threshold = 1   \n",
    "    upper_threshold = 60  \n",
    "\n",
    "    # Filter DataFrame based on outlier criteria\n",
    "    df = df[\n",
    "        (df['duration_minutes'] >= lower_threshold) & \n",
    "        (df['duration_minutes'] <= upper_threshold)\n",
    "    ]\n",
    "\n",
    "    # Define the categorical columns\n",
    "    categorical_features = [\n",
    "        'start_station_id',\n",
    "        'end_station_id'\n",
    "    ]\n",
    "    df[categorical_features] = df[categorical_features].astype(str)\n",
    "    # print(df.shape)\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess(df: pd.DataFrame, dv: DictVectorizer = None, fit_dv: bool = False):\n",
    "    def haversine_distance(row):\n",
    "        lat1, lon1, lat2, lon2 = row['start_lat'], row['start_lng'], row['end_lat'], row['end_lng']\n",
    "        # Convert latitude and longitude from degrees to radians\n",
    "        lat1_rad, lon1_rad, lat2_rad, lon2_rad = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "        # Radius of the Earth in kilometers\n",
    "        radius = 6371.0\n",
    "\n",
    "        # Haversine formula\n",
    "        dlat = lat2_rad - lat1_rad\n",
    "        dlon = lon2_rad - lon1_rad\n",
    "        a = np.sin(dlat / 2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2)**2\n",
    "        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "        distance = radius * c    \n",
    "        return distance\n",
    "\n",
    "    \"\"\"Add features to the model\"\"\"\n",
    "    # Add location ID\n",
    "    df['start_to_end_station_id'] = df['start_station_id'] + '_' + df['end_station_id']\n",
    "    categorical = [\"start_to_end_station_id\"]\n",
    "\n",
    "    # Calc Distance\n",
    "    df['trip_distance'] = df.apply(haversine_distance, axis=1).fillna(0)\n",
    "    numerical   = ['trip_distance']\n",
    "    dicts       = df[categorical + numerical].to_dict(orient='records')\n",
    "\n",
    "    if fit_dv:\n",
    "        # return sparse matrix\n",
    "        dv = DictVectorizer()\n",
    "        X = dv.fit_transform(dicts)\n",
    "    else:\n",
    "        X = dv.transform(dicts)\n",
    "        \n",
    "    # Convert X the sparse matrix  to pandas DataFrame, but too slow\n",
    "    # X = pd.DataFrame(X.toarray(), columns=dv.get_feature_names_out())\n",
    "    # X = pd.DataFrame.sparse.from_spmatrix(X, columns=dv.get_feature_names_out())\n",
    "\n",
    "    try:\n",
    "        # Extract the target\n",
    "        target = 'member_casual'\n",
    "        y = df[target].values\n",
    "    except Exception as e:\n",
    "        print(\"In preprocess Something Wrong...\", e)\n",
    "        pass\n",
    "    # print(X.shape, y.shape)\n",
    "    return (X, y), dv\n",
    "\n",
    "\n",
    "def dump_pickle(obj, filename: str, dest_path: str): \n",
    "    file_path = os.path.join(dest_path, filename)\n",
    "       \n",
    "    # Create dest_path folder unless it already exists\n",
    "    os.makedirs(dest_path, exist_ok=True)\n",
    "    with open(file_path, \"wb\") as f_out:\n",
    "        return pickle.dump(obj, f_out)              \n",
    "                \n",
    "\n",
    "def run_data_prep(raw_data_path=\"./data\", dest_path=\"./output\", location=\"JC-\", years=\"2023\", months=\"5 6 7\") -> None:  \n",
    "    # parameters\n",
    "    locations = location.split(',')\n",
    "    years     = [int(year) for year in years.split()]\n",
    "    months    = [int(month) for month in months.split()]\n",
    "    # print(locations, years, months)\n",
    "\n",
    "    # Download data  \n",
    "    download_data(raw_data_path, locations, years, months)\n",
    "    # print(sorted(glob(f'./data/*')))\n",
    "    \n",
    "    # Load csv files\n",
    "    df_train = read_data(\n",
    "        os.path.join(raw_data_path, f'{locations[0]}{years[0]}{months[0]:0>2}-citibike-tripdata.csv')\n",
    "    )\n",
    "    df_val = read_data(\n",
    "        os.path.join(raw_data_path, f'{locations[0]}{years[0]}{months[1]:0>2}-citibike-tripdata.csv')\n",
    "    )\n",
    "    df_test = read_data(\n",
    "        os.path.join(raw_data_path, f'{locations[0]}{years[0]}{months[2]:0>2}-citibike-tripdata.csv')\n",
    "    )\n",
    "\n",
    "    # Fit the DictVectorizer and preprocess data\n",
    "    (X_train, y_train), dv = preprocess(df_train, fit_dv=True)\n",
    "    (X_val, y_val)    , _  = preprocess(df_val, dv)\n",
    "    (X_test, y_test)  , _  = preprocess(df_test, dv)\n",
    "\n",
    "    # Save DictVectorizer and datasets\n",
    "    dump_pickle(dv, \"dv.pkl\", dest_path)\n",
    "    dump_pickle((X_train, y_train), \"train.pkl\", dest_path)\n",
    "    dump_pickle((X_val, y_val), \"val.pkl\", dest_path)\n",
    "    dump_pickle((X_test, y_test), \"test.pkl\", dest_path)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_data_prep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4db333b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./pycode/preprocess_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7e390f",
   "metadata": {},
   "source": [
    "# MLflow, instrument code & view results in minutes\n",
    "\n",
    "- https://mlflow.org/docs/latest/quickstart.html\n",
    "\n",
    "<div style=\"height: 300px; margin: 0; padding: 0;\">\n",
    "  <img src=\"https://mlflow.org/docs/latest/_images/quickstart_tracking_overview.png\" alt=\"quickstart_tracking_overview\" style=\"width: 100%; height: 100%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f204d579",
   "metadata": {},
   "source": [
    "## train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18a4ec0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pycode/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pycode/train.py\n",
    "\n",
    "import os\n",
    "import click\n",
    "import pickle\n",
    "# from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "# import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import mlflow\n",
    "\n",
    "import warnings\n",
    "# Ignore all warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# Filter the specific warning message, MLflow autologging encountered a warning\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"setuptools\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Setuptools is replacing distutils.\")\n",
    "\n",
    "\n",
    "def load_pickle(\n",
    "    filename: str, data_path: str\n",
    ") -> tuple(\n",
    "    [\n",
    "        scipy.sparse._csr.csr_matrix,\n",
    "        np.ndarray\n",
    "    ]\n",
    "):\n",
    "    file_path = os.path.join(data_path, filename)\n",
    "    with open(file_path, \"rb\") as f_in:\n",
    "        return pickle.load(f_in)\n",
    "\n",
    "\n",
    "def run_train(data_path=\"./output\", dest_path=\"./model\") -> None:\n",
    "    \"\"\"The main training pipeline\"\"\" \n",
    "    # Load train and test Data\n",
    "    X_train, y_train = load_pickle(\"train.pkl\", data_path)\n",
    "    X_val, y_val     = load_pickle(\"val.pkl\", data_path)\n",
    "    # print(type(X_train), type(y_train))\n",
    "\n",
    "    # MLflow settings\n",
    "    # Build or Connect Database Offline\n",
    "    mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "    # Connect Database Online\n",
    "    # mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "    \n",
    "    # Build or Connect mlflow experiment\n",
    "    EXPERIMENT_NAME = \"random-forest-train\"\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "            \n",
    "    # before your training code to enable automatic logging of sklearn metrics, params, and models\n",
    "    # mlflow.sklearn.autolog()    \n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        # Optional: Set some information about Model\n",
    "        mlflow.set_tag(\"developer\", \"muce\")\n",
    "        mlflow.set_tag(\"algorithm\", \"Machine Learning\")\n",
    "        mlflow.set_tag(\"train-data-path\", f'{data_path}/train.pkl')\n",
    "        mlflow.set_tag(\"valid-data-path\", f'{data_path}/val.pkl')\n",
    "        mlflow.set_tag(\"test-data-path\",  f'{data_path}/test.pkl')        \n",
    "        \n",
    "        # Set Model params information\n",
    "        params = {\"max_depth\": 9, \"class_weight\": \"balanced\", \"random_state\": 42}\n",
    "        mlflow.log_params(params)\n",
    "        \n",
    "        # Build Model        \n",
    "        rf     = RandomForestClassifier(**params)\n",
    "        rf.fit(X_train, y_train)\n",
    "        \n",
    "        # autolog_run = mlflow.last_active_run()\n",
    "\n",
    "        # Log the validation Metric to the tracking server\n",
    "        y_pred = rf.predict(X_val)\n",
    "        print(classification_report(y_val, y_pred))\n",
    "\n",
    "        pr_fscore   = precision_recall_fscore_support(y_val, y_pred, average='weighted')\n",
    "        # Extract the F1-score from the tuple\n",
    "        weighted_f1_score = pr_fscore[2]\n",
    "        mlflow.log_metric(\"weighted_f1_score\", weighted_f1_score)\n",
    "        # print(\"weighted_f1_score\", weighted_f1_score)\n",
    "                        \n",
    "        # Log Model two options\n",
    "        # Option1: Just only model in log\n",
    "        mlflow.sklearn.log_model(sk_model = rf, artifact_path = \"model_mlflow\")\n",
    "                \n",
    "        # Option 2: save Model, and Optional: Preprocessor or Pipeline in log\n",
    "        # Create dest_path folder unless it already exists\n",
    "        # pathlib.Path(dest_path).mkdir(exist_ok=True)\n",
    "        os.makedirs(dest_path, exist_ok=True)\n",
    "        local_path = os.path.join(dest_path, \"ride_duration_rf_model.bin\")\n",
    "        with open(local_path, 'wb') as f_out:\n",
    "            pickle.dump(rf, f_out)\n",
    "            \n",
    "        # whole proccess like pickle, saved Model, Optional: Preprocessor or Pipeline\n",
    "        mlflow.log_artifact(local_path = local_path, artifact_path=\"model_pickle\")        \n",
    "        \n",
    "        print(f\"default artifacts URI: '{mlflow.get_artifact_uri()}'\")\n",
    "    return None\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de29e33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      casual       0.36      0.61      0.45     26245\n",
      "      member       0.79      0.57      0.66     66786\n",
      "\n",
      "    accuracy                           0.58     93031\n",
      "   macro avg       0.57      0.59      0.55     93031\n",
      "weighted avg       0.66      0.58      0.60     93031\n",
      "\n",
      "default artifacts URI: 'file:///c:/Users/clk/Jupyter_Notebook/MLOps-Zoomcamp-Module-1-MLOps-Introduction - Kopya/solution-module-7-project-2/mlruns/1/9f4e28cce997415a8f480cd361cbf48f/artifacts'\n"
     ]
    }
   ],
   "source": [
    "!python ./pycode/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d76be17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow ui  --backend-store-uri sqlite:///./mlflow.db --host localhost --port 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa73dc9",
   "metadata": {},
   "source": [
    "## hpo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a9ebbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pycode/hpo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pycode/hpo.py\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import click\n",
    "# from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "import mlflow\n",
    "\n",
    "\n",
    "def load_pickle(\n",
    "    filename: str, data_path: str\n",
    ") -> tuple(\n",
    "    [\n",
    "        scipy.sparse._csr.csr_matrix,\n",
    "        np.ndarray\n",
    "    ]\n",
    "):\n",
    "    file_path = os.path.join(data_path, filename)\n",
    "    with open(file_path, \"rb\") as f_in:\n",
    "        return pickle.load(f_in)\n",
    "\n",
    "\n",
    "def run_optimization(data_path=\"./output\", num_trials=10) -> None:\n",
    "    \"\"\"The main optimization pipeline\"\"\"\n",
    "    # Load train and test Data\n",
    "    X_train, y_train = load_pickle(\"train.pkl\", data_path)\n",
    "    X_val, y_val     = load_pickle(\"val.pkl\", data_path) \n",
    "    # print(type(X_train), type(y_train))\n",
    "    \n",
    "    # MLflow settings\n",
    "    # Build or Connect Database Offline\n",
    "    mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "    # Connect Database Online\n",
    "    # mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "    \n",
    "    # Build or Connect mlflow experiment\n",
    "    HPO_EXPERIMENT_NAME = \"random-forest-hyperopt\"\n",
    "    mlflow.set_experiment(HPO_EXPERIMENT_NAME)\n",
    "    \n",
    "    # before your training code to disable automatic logging of sklearn metrics, params, and models\n",
    "    mlflow.sklearn.autolog(disable=True)\n",
    "\n",
    "    # Optional: Set some information about Model\n",
    "    mlflow.set_tag(\"developer\", \"muce\")\n",
    "    mlflow.set_tag(\"algorithm\", \"Machine Learning\")\n",
    "    mlflow.set_tag(\"train-data-path\", f'{data_path}/train.pkl')\n",
    "    mlflow.set_tag(\"valid-data-path\", f'{data_path}/val.pkl')\n",
    "    mlflow.set_tag(\"test-data-path\",  f'{data_path}/test.pkl')\n",
    "\n",
    "    def objective(trial):\n",
    "        with mlflow.start_run(nested=True):\n",
    "            # Define params            \n",
    "            params = {\n",
    "                \"class_weight\"     : trial.suggest_categorical(name='class_weight', choices=[\"balanced\"]),\n",
    "                'n_estimators'     : trial.suggest_int('n_estimators', 70, 120, 10),\n",
    "                'max_depth'        : trial.suggest_int('max_depth', 3, 21, 2),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 17, 3),\n",
    "                'min_samples_leaf' : trial.suggest_int('min_samples_leaf', 1, 6, 1),\n",
    "                'random_state'     : 42,\n",
    "                'n_jobs'           : -1\n",
    "            }            \n",
    "            # Log the model params to the tracking server\n",
    "            mlflow.log_params(params)\n",
    "            \n",
    "            # Build Model   \n",
    "            rf = RandomForestClassifier(**params)\n",
    "            rf.fit(X_train, y_train)\n",
    "            \n",
    "            # Log the validation Metric to the tracking server\n",
    "            y_pred_val = rf.predict(X_val)\n",
    "            # print(classification_report(y_val, y_pred))\n",
    "\n",
    "            pr_fscore_val   = precision_recall_fscore_support(y_val, y_pred_val, average='weighted')\n",
    "            # Extract the F1-score from the tuple\n",
    "            weighted_f1_score_val = pr_fscore_val[2]\n",
    "            mlflow.log_metric(\"weighted_f1_score_val\", weighted_f1_score_val)\n",
    "            # print(\"weighted_f1_score_val\", weighted_f1_score_val)\n",
    "        return weighted_f1_score_val\n",
    "\n",
    "    sampler = TPESampler(seed=42)\n",
    "    study   = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "    study.optimize(func = objective, n_trials=num_trials, n_jobs=-1)\n",
    "    \n",
    "    print(f\"default artifacts URI: '{mlflow.get_artifact_uri()}'\")\n",
    "    return None\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_optimization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b99cad14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default artifacts URI: 'file:///c:/Users/clk/Jupyter_Notebook/MLOps-Zoomcamp-Module-1-MLOps-Introduction - Kopya/solution-module-7-project-2/mlruns/2/67afaa773031406c899410e0bc690ee1/artifacts'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-21 21:45:31,841] A new study created in memory with name: no-name-5795e751-51e4-4124-b2cb-b702929aacdc\n",
      "[I 2023-08-21 21:45:52,352] Trial 1 finished with value: 0.6008153675540351 and parameters: {'class_weight': 'balanced', 'n_estimators': 70, 'max_depth': 15, 'min_samples_split': 2, 'min_samples_leaf': 5}. Best is trial 1 with value: 0.6008153675540351.\n",
      "[I 2023-08-21 21:45:59,207] Trial 4 finished with value: 0.6009985292584102 and parameters: {'class_weight': 'balanced', 'n_estimators': 80, 'max_depth': 15, 'min_samples_split': 14, 'min_samples_leaf': 3}. Best is trial 4 with value: 0.6009985292584102.\n",
      "[I 2023-08-21 21:45:59,530] Trial 0 finished with value: 0.5974334790915297 and parameters: {'class_weight': 'balanced', 'n_estimators': 70, 'max_depth': 21, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 4 with value: 0.6009985292584102.\n",
      "[I 2023-08-21 21:46:00,013] Trial 2 finished with value: 0.6000791528776708 and parameters: {'class_weight': 'balanced', 'n_estimators': 110, 'max_depth': 13, 'min_samples_split': 17, 'min_samples_leaf': 4}. Best is trial 4 with value: 0.6009985292584102.\n",
      "[I 2023-08-21 21:46:03,742] Trial 7 finished with value: 0.6028401664132476 and parameters: {'class_weight': 'balanced', 'n_estimators': 80, 'max_depth': 17, 'min_samples_split': 17, 'min_samples_leaf': 5}. Best is trial 7 with value: 0.6028401664132476.\n",
      "[I 2023-08-21 21:46:07,907] Trial 3 finished with value: 0.601716826538018 and parameters: {'class_weight': 'balanced', 'n_estimators': 100, 'max_depth': 19, 'min_samples_split': 8, 'min_samples_leaf': 1}. Best is trial 7 with value: 0.6028401664132476.\n",
      "[I 2023-08-21 21:46:10,019] Trial 5 finished with value: 0.6015360616741345 and parameters: {'class_weight': 'balanced', 'n_estimators': 110, 'max_depth': 15, 'min_samples_split': 5, 'min_samples_leaf': 5}. Best is trial 7 with value: 0.6028401664132476.\n",
      "[I 2023-08-21 21:46:12,098] Trial 6 finished with value: 0.6062174513937887 and parameters: {'class_weight': 'balanced', 'n_estimators': 120, 'max_depth': 21, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 6 with value: 0.6062174513937887.\n",
      "[I 2023-08-21 21:46:14,138] Trial 9 finished with value: 0.6007097109205802 and parameters: {'class_weight': 'balanced', 'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 11, 'min_samples_leaf': 3}. Best is trial 6 with value: 0.6062174513937887.\n",
      "[I 2023-08-21 21:46:16,716] Trial 8 finished with value: 0.6046223208484927 and parameters: {'class_weight': 'balanced', 'n_estimators': 120, 'max_depth': 17, 'min_samples_split': 14, 'min_samples_leaf': 1}. Best is trial 6 with value: 0.6062174513937887.\n"
     ]
    }
   ],
   "source": [
    "!python ./pycode/hpo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9a7b4b",
   "metadata": {},
   "source": [
    "## register_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2b99356a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pycode/register_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pycode/register_model.py\n",
    "\n",
    "import os\n",
    "import click\n",
    "import pickle\n",
    "# from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import mlflow\n",
    "from mlflow.entities import ViewType\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "import warnings\n",
    "# Ignore all warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# Filter the specific warning message, MLflow autologging encountered a warning\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"setuptools\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Setuptools is replacing distutils.\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Distutils was imported before Setuptools,*\")\n",
    "\n",
    "\n",
    "def load_pickle(\n",
    "    filename: str, data_path: str\n",
    ") -> tuple(\n",
    "    [\n",
    "        scipy.sparse._csr.csr_matrix,\n",
    "        np.ndarray\n",
    "    ]\n",
    "):\n",
    "    file_path = os.path.join(data_path, filename)\n",
    "    with open(file_path, \"rb\") as f_in:\n",
    "        return pickle.load(f_in)\n",
    "\n",
    "\n",
    "def train_and_log_model(data_path, params, experiment_name): \n",
    "    \"\"\"The main training pipeline\"\"\"    \n",
    "    # Load train, val and test Data\n",
    "    X_train, y_train = load_pickle(\"train.pkl\", data_path)\n",
    "    X_val, y_val     = load_pickle(\"val.pkl\", data_path)\n",
    "    X_test, y_test   = load_pickle(\"test.pkl\", data_path)\n",
    "    # print(type(X_train), type(y_train))\n",
    "    \n",
    "    # MLflow settings\n",
    "    # Build or Connect Database Offline\n",
    "    mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "    # Connect Database Online\n",
    "    # mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "    \n",
    "    # Build or Connect mlflow experiment\n",
    "    EXPERIMENT_NAME = experiment_name\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "    \n",
    "    # before your training code to enable automatic logging of sklearn metrics, params, and models\n",
    "    # mlflow.sklearn.autolog()\n",
    "\n",
    "    with mlflow.start_run(nested=True):\n",
    "        # Optional: Set some information about Model\n",
    "        mlflow.set_tag(\"developer\", \"muce\")\n",
    "        mlflow.set_tag(\"algorithm\", \"Machine Learning\")\n",
    "        mlflow.set_tag(\"train-data-path\", f'{data_path}/train.pkl')\n",
    "        mlflow.set_tag(\"valid-data-path\", f'{data_path}/val.pkl')\n",
    "        mlflow.set_tag(\"test-data-path\",  f'{data_path}/test.pkl')  \n",
    "\n",
    "        # Set Model params information\n",
    "        RF_PARAMS = ['n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'random_state', 'n_jobs']\n",
    "        for param in RF_PARAMS:\n",
    "            params[param] = int(params[param])\n",
    "            \n",
    "        # Log the model params to the tracking server\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        # Build Model\n",
    "        rf = RandomForestClassifier(**params)\n",
    "        rf.fit(X_train, y_train)\n",
    "\n",
    "        # Log the validation and test Metric to the tracking server\n",
    "        y_pred_val = rf.predict(X_val)\n",
    "        y_pred_test = rf.predict(X_test)\n",
    "        pr_fscore_val   = precision_recall_fscore_support(y_val, y_pred_val, average='weighted')\n",
    "        pr_fscore_test   = precision_recall_fscore_support(y_test, y_pred_test, average='weighted')\n",
    "        # Extract the F1-score from the tuple\n",
    "        weighted_f1_score_val = pr_fscore_val[2]\n",
    "        weighted_f1_score_test = pr_fscore_test[2]\n",
    "        mlflow.log_metric(\"weighted_f1_score_val\", weighted_f1_score_val)\n",
    "        mlflow.log_metric(\"weighted_f1_score_test\", weighted_f1_score_test)\n",
    "        # print(\"weighted_f1_score_val\", weighted_f1_score_val)\n",
    "\n",
    "        # Log the model\n",
    "        # Option1: Just only model in log\n",
    "        mlflow.sklearn.log_model(sk_model = rf, artifact_path = \"model_mlflow\")\n",
    "        \n",
    "        # print(f\"default artifacts URI: '{mlflow.get_artifact_uri()}'\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def run_register_model(data_path=\"./output\", top_n=5) -> None:\n",
    "    \"\"\"The main optimization pipeline\"\"\"\n",
    "    # Parameters\n",
    "    EXPERIMENT_NAME = \"random-forest-best-models\"\n",
    "    HPO_EXPERIMENT_NAME = \"random-forest-hyperopt\"\n",
    "    client = MlflowClient(\"sqlite:///mlflow.db\")\n",
    "\n",
    "    # Retrieve the top_n model runs and log the models\n",
    "    experiment = client.get_experiment_by_name(HPO_EXPERIMENT_NAME)\n",
    "    runs = client.search_runs(\n",
    "        experiment_ids=experiment.experiment_id,\n",
    "        run_view_type=ViewType.ACTIVE_ONLY,\n",
    "        max_results=top_n,\n",
    "        order_by=[\"metrics.weighted_f1_score_val DESC\"]\n",
    "    )\n",
    "    for run in runs:\n",
    "        train_and_log_model(data_path=data_path, params=run.data.params, experiment_name=EXPERIMENT_NAME)\n",
    "\n",
    "    # Select the model with the lowest test RMSE\n",
    "    experiment = client.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "    best_run = client.search_runs(\n",
    "        experiment_ids=experiment.experiment_id,\n",
    "        run_view_type=ViewType.ACTIVE_ONLY,\n",
    "        max_results=1,\n",
    "        order_by=[\"metrics.weighted_f1_score_test DESC\"]\n",
    "    )[0]\n",
    "\n",
    "    # Register the best model\n",
    "    run_id     = best_run.info.run_id\n",
    "    model_uri  = f\"runs:/{run_id}/model\"\n",
    "    model_name = \"rf-best-model\"\n",
    "    mlflow.register_model(model_uri, name=model_name)\n",
    "\n",
    "    # print(\"Test weighted_f1_score_test of the best model: {:.4f}\".format(best_run.data.metrics[\"weighted_f1_score_test\"]))\n",
    "    return None\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_register_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3dcc4f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'rf-best-model' already exists. Creating a new version of this model...\n",
      "2023/08/21 22:08:48 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: rf-best-model, version 2\n",
      "Created version '2' of model 'rf-best-model'.\n"
     ]
    }
   ],
   "source": [
    "!python ./pycode/register_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf34682f",
   "metadata": {},
   "source": [
    "# End of The Project: Part 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
