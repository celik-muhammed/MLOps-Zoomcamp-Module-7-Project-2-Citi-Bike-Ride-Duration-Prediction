{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2525fb2f",
   "metadata": {},
   "source": [
    "<div style=\"align: center; margin: 0; padding: 0; height: 250px;\">\n",
    "    <br>\n",
    "    <img src=\"https://images.ctfassets.net/p6ae3zqfb1e3/3pQAQO2G3wrOcyuCXFbAJd/01f1309dda5327d03a76a051f98f44ac/Citi_Bike_Homepage_Hero_3x__1_.jpg\" style=\"display:block; margin:auto; width:65%; height:100%;\">\n",
    "</div><br><br>\n",
    "\n",
    "<div style=\"letter-spacing:normal; opacity:1.;\">\n",
    "<!--   https://xkcd.com/color/rgb/   -->\n",
    "  <p style=\"text-align:center; background-color: lightsalmon; color: Jaguar; border-radius:10px; font-family:monospace; \n",
    "            line-height:1.4; font-size:32px; font-weight:bold; text-transform: uppercase; padding: 9px;\">\n",
    "            <strong>Citi Bike Trip History Data</strong></p>  \n",
    "  \n",
    "  <p style=\"text-align:center; background-color:romance; color: Jaguar; border-radius:10px; font-family:monospace; \n",
    "            line-height:1.4; font-size:22px; font-weight:normal; text-transform: capitalize; padding: 5px;\"\n",
    "     >Machine Learning Module: Ride Duration Prediction using Regression Analysis<br>(EDA and Linear Regression Model Training)</p>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a8b00d",
   "metadata": {},
   "source": [
    "**Dataset Info**\n",
    "\n",
    "> https://citibikenyc.com/system-data\n",
    "\n",
    "**System Data**\n",
    "\n",
    "Where do Citi Bikers ride? When do they ride? How far do they go? Which stations are most popular? What days of the week are most rides taken on? We've heard all of these questions and more from you, and we're happy to provide the data to help you discover the answers to these questions and more. We invite developers, engineers, statisticians, artists, academics and other interested members of the public to use the data we provide for analysis, development, visualization and whatever else moves you.\n",
    "\n",
    "This data is provided according to the [NYCBS Data Use Policy](https://www.citibikenyc.com/data-sharing-policy).\n",
    "\n",
    "\n",
    "**Citi Bike Trip Histories**\n",
    "\n",
    "We publish [downloadable files of Citi Bike trip data](https://s3.amazonaws.com/tripdata/index.html). The data includes:\n",
    "\n",
    "- Ride ID\n",
    "- Rideable type\n",
    "- Started at\n",
    "- Ended at\n",
    "- Start station name\n",
    "- Start station ID\n",
    "- End station name\n",
    "- End station ID\n",
    "- Start latitude\n",
    "- Start longitude\n",
    "- End latitude\n",
    "- End Longitude\n",
    "- Member or casual ride\n",
    "\n",
    "Data format previously:\n",
    "\n",
    "- Trip Duration (seconds)\n",
    "- Start Time and Date\n",
    "- Stop Time and Date\n",
    "- Start Station Name\n",
    "- End Station Name\n",
    "- Station ID\n",
    "- Station Lat/Long\n",
    "- Bike ID\n",
    "- User Type (Customer = 24-hour pass or 3-day pass user; Subscriber = Annual Member)\n",
    "- Gender (Zero=unknown; 1=male; 2=female)\n",
    "- Year of Birth\n",
    "\n",
    "This data has been processed to remove trips that are taken by staff as they service and inspect the system, trips that are taken to/from any of our “test” stations (which we were using more in June and July 2013), and any trips that were below 60 seconds in length (potentially false starts or users trying to re-dock a bike to ensure it's secure).\n",
    "\n",
    "[Download Citi Bike trip history data](https://s3.amazonaws.com/tripdata/index.html)\n",
    "\n",
    "**Monthly Operating Reports**\n",
    "\n",
    "View the [monthly operating reports](https://www.citibikenyc.com/system-data/operating-reports) that we provide to the NYC Department of Transportation.\n",
    "\n",
    "**Additional Resources**\n",
    "\n",
    "- The City of New York's [bicycling data](http://www.nyc.gov/html/dot/html/about/datafeeds.shtml#Bikes)\n",
    "- A group of software developers and data explorers working with data feeds from NYC's Bike Share system and other bike data maintain this [Google Group](https://groups.google.com/forum/#!aboutgroup/citibike-hackers) (note: Citi Bike is not responsible for this group – it is run and maintained by a group of interested private citizens)\n",
    "\n",
    "**TASK**\n",
    "\n",
    "The goal of this project is to apply everything we learned in this course and build an end-to-end machine learning project.\n",
    "\n",
    "Remember that to pass the project, you must evaluate 3 peers. If you don't do that, your project can't be considered compelete.\n",
    "\n",
    "Submitting\n",
    "\n",
    "Project Cohort #2\n",
    "\n",
    "Project:\n",
    "\n",
    "- Form: https://forms.gle/o1s3NmYE4UmFSMVD7\n",
    "- Deadline: 21 August (Monday), 23:00 CEST\n",
    "\n",
    "Peer reviewing:\n",
    "\n",
    "- Peer review assignments: TBA (\"project 2\" tab)\n",
    "- Form: TBA\n",
    "-  Deadline: TBA\n",
    "\n",
    "Project feedback: TBA (\"feedback-02\" tab)\n",
    "\n",
    "Evaluation criteria\n",
    "See [here](https://github.com/DataTalksClub/mlops-zoomcamp/blob/main/07-project/README.md)\n",
    "\n",
    "Questions: https://github.com/DataTalksClub/mlops-zoomcamp/tree/main/cohorts/2023/07-project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78138c11",
   "metadata": {},
   "source": [
    "<div style=\"letter-spacing:normal; opacity:1.;\">\n",
    "  <h1 style=\"text-align:center; background-color: lightsalmon; color: Jaguar; border-radius:10px; font-family:monospace; border-radius:20px;\n",
    "            line-height:1.4; font-size:32px; font-weight:bold; text-transform: uppercase; padding: 9px;\">\n",
    "            <strong>1. Import Libraries & Ingest Data</strong></h1>   \n",
    "</div>\n",
    "\n",
    "> ⚠️ Not Recommended conda `base` env, work on `venv`\n",
    "\n",
    "- https://docs.conda.io/projects/conda/en/4.6.0/_downloads/52a95608c49671267e40c689e0bc00ca/conda-cheatsheet.pdf\n",
    "\n",
    "```\n",
    "pip freeze > requirements.txt\n",
    "conda list -e > requirements.txt\n",
    "\n",
    "# new conda virtual environment\n",
    "conda create --name \"lin-reg\" python=3.10 jupyter -y\n",
    "conda activate \"lin-reg\"\n",
    "\n",
    "# install all package dependencies\n",
    "pip install -r requirements.txt\n",
    "conda install -c conda-forge --file=requirements.txt      # mostly not work\n",
    "conda install -c conda-forge pandas==2.0.2 -q -y\n",
    "\n",
    "# if The environment is inconsistent, try below\n",
    "conda update -n base -c defaults conda --force-reinstall\n",
    "conda install anaconda --force-reinstall\n",
    "\n",
    "```\n",
    "\n",
    "**You must use the `--no-deps` option in the pip install command in order to avoid bundling dependencies into your conda-package.**\n",
    "\n",
    "If you run pip install without the `--no-deps` option, pip will often install dependencies in your conda recipe and those dependencies will become part of your package. This wastes space in the package and `increases the risk of file overlap`, file clobbering, and broken packages.\n",
    "\n",
    "There might be cases where you want to install a package directly from a local directory or a specific location, without relying on the package indexes. In such situations, you can use the `--no-index` option to tell pip not to look for the package in any indexes.\n",
    "\n",
    "```\n",
    "- command1 & command2  # runs simultaneously\n",
    "- command1 ; command2  # runs sequentially\n",
    "- command1 && command2 # runs sequentially, runs command2 only if command1 succeeds\n",
    "- command1 || command2 # runs sequentially, runs command2 only if command1 fails\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81ffeeb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy\n",
    "from scipy.sparse import hstack\n",
    "from scipy.stats import stats\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "# from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "# from sklearn.ensemble import GradientBoostingRegressor, ExtraTreesRegressor\n",
    "# from sklearn.svm import LinearSVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import zipfile\n",
    "# import boto3\n",
    "# import click\n",
    "# import pathlib\n",
    "# import argparse\n",
    "# import requests\n",
    "# import urllib.request\n",
    "from glob import glob\n",
    "# from tqdm import tqdm           # console-based\n",
    "# from tqdm.notebook import tqdm  # jupyter-based\n",
    "from tqdm.auto import tqdm        # automatically selects\n",
    "# tqdm._instances.clear()\n",
    "\n",
    "import mlflow\n",
    "import wandb\n",
    "\n",
    "import prefect\n",
    "from prefect import task, flow, Flow\n",
    "from prefect.tasks import task_input_hash\n",
    "from prefect.artifacts import create_markdown_artifact\n",
    "\n",
    "# memory management performs garbage collection \n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d870c290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlflow.__version__:  2.5.0\n",
      "wandb.__version__ :  0.15.8\n"
     ]
    }
   ],
   "source": [
    "# from mlflow.tracking import MlflowClient\n",
    "# from mlflow.exceptions import MlflowException\n",
    "import mlflow; print(\"mlflow.__version__: \", mlflow.__version__)\n",
    "import wandb;  print(\"wandb.__version__ : \", wandb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95b6c20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:             2.11.3\n",
      "API version:         0.8.4\n",
      "Python version:      3.10.12\n",
      "Git commit:          3a400865\n",
      "Built:               Thu, Aug 3, 2023 3:24 PM\n",
      "OS/Arch:             win32/AMD64\n",
      "Profile:             default\n",
      "Server type:         ephemeral\n",
      "Server:\n",
      "  Database:          sqlite\n",
      "  SQLite version:    3.41.2\n"
     ]
    }
   ],
   "source": [
    "!prefect version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e78613e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Get the current working directory\n",
    "# current_dir = os.getcwd()\n",
    "\n",
    "# Create a new directory for storing data\n",
    "os.makedirs('./pycode', exist_ok=True)\n",
    "# os.makedirs('./data', exist_ok=True)\n",
    "# os.makedirs('./output', exist_ok=True)\n",
    "# os.makedirs('./model', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "044ce591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data\\\\JC-202305-citibike-tripdata.csv',\n",
       " 'data\\\\JC-202305-citibike-tripdata.csv.zip',\n",
       " 'data\\\\JC-202306-citibike-tripdata.csv',\n",
       " 'data\\\\JC-202306-citibike-tripdata.csv.zip',\n",
       " 'data\\\\JC-202307-citibike-tripdata.csv',\n",
       " 'data\\\\JC-202307-citibike-tripdata.csv.zip']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob('data/*.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcc106e",
   "metadata": {},
   "source": [
    "## orchestrate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c861a633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pycode/orchestrate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pycode/orchestrate.py\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import zipfile\n",
    "\n",
    "# import pathlib\n",
    "import requests\n",
    "import urllib.request\n",
    "from glob import glob\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import mlflow\n",
    "import prefect\n",
    "from prefect import task, flow\n",
    "from prefect.tasks import task_input_hash\n",
    "from prefect.artifacts import create_markdown_artifact\n",
    "\n",
    "# from prefect_aws import S3Bucket\n",
    "# from prefect_email import EmailServerCredentials, email_send_message\n",
    "\n",
    "import warnings\n",
    "# Ignore all warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# Filter the specific warning message, MLflow autologging encountered a warning\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"setuptools\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Setuptools is replacing distutils.\")\n",
    "\n",
    "\n",
    "@task(name=\"Fetch Data\", cache_key_fn=task_input_hash, cache_expiration=timedelta(days=1),\n",
    "      retries=3, log_prints=True, )\n",
    "def fetch_data(raw_data_path: str, location: str, year: int, month: int) -> None:\n",
    "    \"\"\"Fetches data from the NYC Taxi dataset and saves it locally\"\"\"\n",
    "    filename = f'{location}{year}{month:0>2}-citibike-tripdata.csv.zip'\n",
    "    filepath = os.path.join(raw_data_path, filename)\n",
    "    url      = f'https://s3.amazonaws.com/tripdata/{filename}'\n",
    "\n",
    "    # Create dest_path folder unless it already exists\n",
    "    os.makedirs(raw_data_path, exist_ok=True)\n",
    "    \n",
    "    # Download the data from the NYC Taxi dataset\n",
    "    # os.system(f\"wget -q -N -P {raw_data_path} {url}\")\n",
    "    # urllib.request.urlretrieve(url, filename)\n",
    "    response = requests.get(url)\n",
    "    with open(filepath, \"wb\") as f_out:\n",
    "        f_out.write(response.content)\n",
    "    \n",
    "    # Extract the CSV file from the ZIP file\n",
    "    with zipfile.ZipFile(filepath, 'r') as zip_ref:\n",
    "        zip_ref.extract(filename.replace('.zip', ''), path=raw_data_path)  # Extract to a specific directory\n",
    "    return None\n",
    "\n",
    "\n",
    "@flow(name=\"Data Downloads\", log_prints=True)\n",
    "def download_data(raw_data_path: str, locations: list, years: list, months: list) -> None:\n",
    "    try:\n",
    "        # Download the data from the NYC Taxi dataset    \n",
    "        for loc in locations: \n",
    "            for year in years:       \n",
    "                for month in months:\n",
    "                    fetch_data(raw_data_path, loc, year, month)\n",
    "    except Exception as e:\n",
    "        print(\"In download_data Something Wrong...\", e)\n",
    "        pass\n",
    "    return None\n",
    "    \n",
    "    \n",
    "@task(name=\"Read Data\", retries=3, retry_delay_seconds=2, log_prints=None)\n",
    "def read_data(filename: str) -> pd.DataFrame:\n",
    "    \"\"\"Read data into DataFrame\"\"\"\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    # Convert Datetime\n",
    "    df['started_at'] = pd.to_datetime(df['started_at'])\n",
    "    df['ended_at']   = pd.to_datetime(df['ended_at'])\n",
    "\n",
    "    # Calculate duration, Convert duration to minutes\n",
    "    df['duration'] = df['ended_at'] - df['started_at']\n",
    "    df['duration_minutes'] = df['duration'].dt.total_seconds() / 60\n",
    "\n",
    "    # Define criteria for outliers \n",
    "    lower_threshold = 1   \n",
    "    upper_threshold = 60  \n",
    "\n",
    "    # Filter DataFrame based on outlier criteria\n",
    "    df = df[\n",
    "        (df['duration_minutes'] >= lower_threshold) & \n",
    "        (df['duration_minutes'] <= upper_threshold)\n",
    "    ]\n",
    "\n",
    "    # Define the categorical columns\n",
    "    categorical_features = [\n",
    "        'start_station_id',\n",
    "        'end_station_id'\n",
    "    ]\n",
    "    df[categorical_features] = df[categorical_features].astype(str)\n",
    "    # print(df.shape)\n",
    "    return df\n",
    "\n",
    "\n",
    "@task(name=\"Preprocess: Add Features\", log_prints=True)\n",
    "def preprocess(\n",
    "    df: pd.DataFrame,dv: DictVectorizer = None, fit_dv: bool = False\n",
    ") -> tuple(\n",
    "    [\n",
    "        scipy.sparse._csr.csr_matrix,\n",
    "        np.ndarray,\n",
    "        sklearn.feature_extraction.DictVectorizer,\n",
    "    ]\n",
    "):\n",
    "    def haversine_distance(row):\n",
    "        lat1, lon1, lat2, lon2 = row['start_lat'], row['start_lng'], row['end_lat'], row['end_lng']\n",
    "        # Convert latitude and longitude from degrees to radians\n",
    "        lat1_rad, lon1_rad, lat2_rad, lon2_rad = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "        # Radius of the Earth in kilometers\n",
    "        radius = 6371.0\n",
    "\n",
    "        # Haversine formula\n",
    "        dlat = lat2_rad - lat1_rad\n",
    "        dlon = lon2_rad - lon1_rad\n",
    "        a = np.sin(dlat / 2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2)**2\n",
    "        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "        distance = radius * c    \n",
    "        return distance\n",
    "\n",
    "    \"\"\"Add features to the model\"\"\"\n",
    "    # Add location ID\n",
    "    df['start_to_end_station_id'] = df['start_station_id'] + '_' + df['end_station_id']\n",
    "    categorical = [\"start_to_end_station_id\"]\n",
    "\n",
    "    # Calc Distance\n",
    "    df['trip_distance'] = df.apply(haversine_distance, axis=1).fillna(0)\n",
    "    numerical   = ['trip_distance']\n",
    "    dicts       = df[categorical + numerical].to_dict(orient='records')\n",
    "\n",
    "    if fit_dv:\n",
    "        # return sparse matrix\n",
    "        dv = DictVectorizer()\n",
    "        X = dv.fit_transform(dicts)\n",
    "    else:\n",
    "        X = dv.transform(dicts)\n",
    "        \n",
    "    # Convert X the sparse matrix  to pandas DataFrame, but too slow\n",
    "    # X = pd.DataFrame(X.toarray(), columns=dv.get_feature_names_out())\n",
    "    # X = pd.DataFrame.sparse.from_spmatrix(X, columns=dv.get_feature_names_out())\n",
    "\n",
    "    try:\n",
    "        # Extract the target\n",
    "        target = 'member_casual'\n",
    "        y = df[target].values\n",
    "    except Exception as e:\n",
    "        print(\"In preprocess Something Wrong...\", e)\n",
    "        pass\n",
    "    # print(X.shape, y.shape)\n",
    "    return (X, y), dv\n",
    "\n",
    "\n",
    "@task(name=\"Train Best Model\", log_prints=True)\n",
    "def train_best_model(\n",
    "    X_train  : scipy.sparse._csr.csr_matrix,\n",
    "    X_val    : scipy.sparse._csr.csr_matrix,\n",
    "    y_train  : np.ndarray,\n",
    "    y_val    : np.ndarray,\n",
    "    dv       : sklearn.feature_extraction.DictVectorizer,\n",
    "    raw_data_path: str,\n",
    "    dest_path: str,\n",
    ") -> None:\n",
    "    \"\"\"train a model with best hyperparams and write everything out\"\"\"   \n",
    "    # Assuming 'y' is your categorical label data\n",
    "    le = LabelEncoder()\n",
    "    y_train_num = le.fit_transform(y_train)\n",
    "    y_val_num = le.transform(y_val)\n",
    "\n",
    "    # Load train and test Data\n",
    "    train = xgb.DMatrix(X_train, label=y_train_num)\n",
    "    valid = xgb.DMatrix(X_val, label=y_val_num)\n",
    "    # print(type(X_train), type(y_train))\n",
    "\n",
    "    # MLflow settings\n",
    "    # Build or Connect Database Offline\n",
    "    mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "    # Connect Database Online\n",
    "    # mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "    \n",
    "    # Build or Connect mlflow experiment\n",
    "    EXPERIMENT_NAME = \"nyc-taxi-experiment\"\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "    # before your training code to enable automatic logging of sklearn metrics, params, and models\n",
    "    # mlflow.xgboost.autolog()\n",
    "    \n",
    "    with mlflow.start_run(nested=True):\n",
    "        # Optional: Set some information about Model\n",
    "        mlflow.set_tag(\"developer\", \"muce\")\n",
    "        mlflow.set_tag(\"algorithm\", \"Machine Learning\")\n",
    "        mlflow.set_tag(\"train-data-path\", f'{raw_data_path}/green_tripdata_2023-01.parquet')\n",
    "        mlflow.set_tag(\"valid-data-path\", f'{raw_data_path}/green_tripdata_2023-02.parquet')\n",
    "        mlflow.set_tag(\"test-data-path\",  f'{raw_data_path}/green_tripdata_2023-03.parquet') \n",
    "        \n",
    "        # Set Model params information\n",
    "        best_params = {\n",
    "            \"learning_rate\": 0.09585355369315604,\n",
    "            \"max_depth\": 30,\n",
    "            \"min_child_weight\": 1.060597050922164,\n",
    "            'objective': 'binary:logistic',\n",
    "            \"reg_alpha\": 0.018060244040060163,\n",
    "            \"reg_lambda\": 0.011658731377413597,\n",
    "            \"seed\": 42,\n",
    "        }\n",
    "        mlflow.log_params(best_params)\n",
    "\n",
    "        # Build Model   \n",
    "        booster = xgb.train(\n",
    "            params               = best_params,\n",
    "            dtrain               = train,\n",
    "            num_boost_round      = 100,\n",
    "            evals                = [(valid, \"validation\")],\n",
    "            early_stopping_rounds=20,\n",
    "        )\n",
    "        # Log the validation Metric to the tracking server\n",
    "        y_pred_val = booster.predict(valid)\n",
    "        threshold = 0.5  # Example threshold value\n",
    "        y_pred_class = (y_pred_val > threshold).astype(int)\n",
    "        print(classification_report(y_val, le.inverse_transform(y_pred_class)))\n",
    "\n",
    "        pr_fscore_val   = precision_recall_fscore_support(y_val, le.inverse_transform(y_pred_class), average='weighted')\n",
    "        # Extract the F1-score from the tuple\n",
    "        weighted_f1_score_val = pr_fscore_val[2]\n",
    "        mlflow.log_metric(\"weighted_f1_score_val\", weighted_f1_score_val)\n",
    "        # print(\"weighted_f1_score\", weighted_f1_score)              \n",
    "\n",
    "        # Log Model two options\n",
    "        # Option1: Just only model in log\n",
    "        mlflow.xgboost.log_model(booster, artifact_path=\"model_mlflow\")        \n",
    "        \n",
    "        # Option 2: save Model, and Optional: Preprocessor or Pipeline in log         \n",
    "        # Create dest_path folder unless it already exists\n",
    "        # pathlib.Path(dest_path).mkdir(exist_ok=True) \n",
    "        os.makedirs(dest_path, exist_ok=True)       \n",
    "        local_file = os.path.join(dest_path, \"preprocessor.b\")\n",
    "        with open(local_file, \"wb\") as f_out:\n",
    "            pickle.dump(dv, f_out)\n",
    "            \n",
    "        # whole proccess like pickle, saved Model, Optional: Preprocessor or Pipeline\n",
    "        mlflow.log_artifact(local_path = local_file, artifact_path=\"preprocessor\")        \n",
    "        \n",
    "        # print(f\"default artifacts URI: '{mlflow.get_artifact_uri()}'\")\n",
    "    return None               \n",
    "\n",
    "\n",
    "@flow(name=\"Main Flow\")\n",
    "def main_flow(raw_data_path=\"./data\", dest_path=\"./output\", location=\"JC-\", years=\"2023\", months=\"5 6 7\") -> None:  \n",
    "    # parameters\n",
    "    locations = location.split(',')\n",
    "    years     = [int(year) for year in years.split()]\n",
    "    months    = [int(month) for month in months.split()]\n",
    "    # print(locations, years, months)\n",
    "\n",
    "    # Download data  \n",
    "    download_data(raw_data_path, locations, years, months)\n",
    "    # print(sorted(glob(f'./data/*')))\n",
    "    \n",
    "    # Load csv files\n",
    "    df_train = read_data(\n",
    "        os.path.join(raw_data_path, f'{locations[0]}{years[0]}{months[0]:0>2}-citibike-tripdata.csv')\n",
    "    )\n",
    "    df_val = read_data(\n",
    "        os.path.join(raw_data_path, f'{locations[0]}{years[0]}{months[1]:0>2}-citibike-tripdata.csv')\n",
    "    )\n",
    "    df_test = read_data(\n",
    "        os.path.join(raw_data_path, f'{locations[0]}{years[0]}{months[2]:0>2}-citibike-tripdata.csv')\n",
    "    )\n",
    "\n",
    "    # Fit the DictVectorizer and preprocess data\n",
    "    (X_train, y_train), dv = preprocess(df_train, fit_dv=True)\n",
    "    (X_val, y_val)    , _  = preprocess(df_val, dv)\n",
    "    (X_test, y_test)  , _  = preprocess(df_test, dv)\n",
    "\n",
    "    # Train\n",
    "    train_best_model(X_train, X_val, y_train, y_val, dv, raw_data_path, dest_path)\n",
    "    return None   \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4db333b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:42:34.076 | INFO    | prefect.engine - Created flow run 'provocative-mouse' for flow 'Main Flow'\n",
      "22:42:34.883 | INFO    | Flow run 'provocative-mouse' - Created subflow run 'red-emu' for flow 'Data Downloads'\n",
      "22:42:35.093 | INFO    | Flow run 'red-emu' - Created task run 'Fetch Data-0' for task 'Fetch Data'\n",
      "22:42:35.095 | INFO    | Flow run 'red-emu' - Executing 'Fetch Data-0' immediately...\n",
      "22:42:35.234 | INFO    | Task run 'Fetch Data-0' - Finished in state Cached(type=COMPLETED)\n",
      "22:42:35.299 | INFO    | Flow run 'red-emu' - Created task run 'Fetch Data-1' for task 'Fetch Data'\n",
      "22:42:35.302 | INFO    | Flow run 'red-emu' - Executing 'Fetch Data-1' immediately...\n",
      "22:42:35.444 | INFO    | Task run 'Fetch Data-1' - Finished in state Cached(type=COMPLETED)\n",
      "22:42:35.510 | INFO    | Flow run 'red-emu' - Created task run 'Fetch Data-2' for task 'Fetch Data'\n",
      "22:42:35.513 | INFO    | Flow run 'red-emu' - Executing 'Fetch Data-2' immediately...\n",
      "22:42:35.659 | INFO    | Task run 'Fetch Data-2' - Finished in state Cached(type=COMPLETED)\n",
      "22:42:35.824 | INFO    | Flow run 'red-emu' - Finished in state Completed('All states completed.')\n",
      "22:42:35.894 | INFO    | Flow run 'provocative-mouse' - Created task run 'Read Data-0' for task 'Read Data'\n",
      "22:42:35.896 | INFO    | Flow run 'provocative-mouse' - Executing 'Read Data-0' immediately...\n",
      "22:42:37.275 | INFO    | Task run 'Read Data-0' - Finished in state Completed()\n",
      "22:42:37.352 | INFO    | Flow run 'provocative-mouse' - Created task run 'Read Data-1' for task 'Read Data'\n",
      "22:42:37.355 | INFO    | Flow run 'provocative-mouse' - Executing 'Read Data-1' immediately...\n",
      "22:42:38.669 | INFO    | Task run 'Read Data-1' - Finished in state Completed()\n",
      "22:42:38.738 | INFO    | Flow run 'provocative-mouse' - Created task run 'Read Data-2' for task 'Read Data'\n",
      "22:42:38.740 | INFO    | Flow run 'provocative-mouse' - Executing 'Read Data-2' immediately...\n",
      "22:42:40.231 | INFO    | Task run 'Read Data-2' - Finished in state Completed()\n",
      "22:42:40.304 | INFO    | Flow run 'provocative-mouse' - Created task run 'Preprocess: Add Features-0' for task 'Preprocess: Add Features'\n",
      "22:42:40.307 | INFO    | Flow run 'provocative-mouse' - Executing 'Preprocess: Add Features-0' immediately...\n",
      "22:42:49.709 | INFO    | Task run 'Preprocess: Add Features-0' - Finished in state Completed()\n",
      "22:42:49.781 | INFO    | Flow run 'provocative-mouse' - Created task run 'Preprocess: Add Features-1' for task 'Preprocess: Add Features'\n",
      "22:42:49.784 | INFO    | Flow run 'provocative-mouse' - Executing 'Preprocess: Add Features-1' immediately...\n",
      "22:42:59.553 | INFO    | Task run 'Preprocess: Add Features-1' - Finished in state Completed()\n",
      "22:42:59.622 | INFO    | Flow run 'provocative-mouse' - Created task run 'Preprocess: Add Features-2' for task 'Preprocess: Add Features'\n",
      "22:42:59.625 | INFO    | Flow run 'provocative-mouse' - Executing 'Preprocess: Add Features-2' immediately...\n",
      "22:43:08.868 | INFO    | Task run 'Preprocess: Add Features-2' - Finished in state Completed()\n",
      "22:43:08.943 | INFO    | Flow run 'provocative-mouse' - Created task run 'Train Best Model-0' for task 'Train Best Model'\n",
      "22:43:08.947 | INFO    | Flow run 'provocative-mouse' - Executing 'Train Best Model-0' immediately...\n",
      "22:43:12.375 | INFO    | Task run 'Train Best Model-0' - [0]    validation-logloss:0.66922\n",
      "22:43:12.654 | INFO    | Task run 'Train Best Model-0' - [1]    validation-logloss:0.64970\n",
      "22:43:12.927 | INFO    | Task run 'Train Best Model-0' - [2]    validation-logloss:0.63372\n",
      "22:43:13.220 | INFO    | Task run 'Train Best Model-0' - [3]    validation-logloss:0.62057\n",
      "22:43:13.527 | INFO    | Task run 'Train Best Model-0' - [4]    validation-logloss:0.60945\n",
      "22:43:13.810 | INFO    | Task run 'Train Best Model-0' - [5]    validation-logloss:0.60040\n",
      "22:43:14.067 | INFO    | Task run 'Train Best Model-0' - [6]    validation-logloss:0.59280\n",
      "22:43:14.449 | INFO    | Task run 'Train Best Model-0' - [7]    validation-logloss:0.58633\n",
      "22:43:14.766 | INFO    | Task run 'Train Best Model-0' - [8]    validation-logloss:0.58112\n",
      "22:43:15.093 | INFO    | Task run 'Train Best Model-0' - [9]    validation-logloss:0.57671\n",
      "22:43:15.414 | INFO    | Task run 'Train Best Model-0' - [10]   validation-logloss:0.57269\n",
      "22:43:15.739 | INFO    | Task run 'Train Best Model-0' - [11]   validation-logloss:0.56962\n",
      "22:43:16.035 | INFO    | Task run 'Train Best Model-0' - [12]   validation-logloss:0.56705\n",
      "22:43:16.367 | INFO    | Task run 'Train Best Model-0' - [13]   validation-logloss:0.56472\n",
      "22:43:16.748 | INFO    | Task run 'Train Best Model-0' - [14]   validation-logloss:0.56304\n",
      "22:43:17.308 | INFO    | Task run 'Train Best Model-0' - [15]   validation-logloss:0.56159\n",
      "22:43:17.746 | INFO    | Task run 'Train Best Model-0' - [16]   validation-logloss:0.56041\n",
      "22:43:18.062 | INFO    | Task run 'Train Best Model-0' - [17]   validation-logloss:0.55949\n",
      "22:43:18.322 | INFO    | Task run 'Train Best Model-0' - [18]   validation-logloss:0.55867\n",
      "22:43:18.643 | INFO    | Task run 'Train Best Model-0' - [19]   validation-logloss:0.55802\n",
      "22:43:18.978 | INFO    | Task run 'Train Best Model-0' - [20]   validation-logloss:0.55747\n",
      "22:43:19.403 | INFO    | Task run 'Train Best Model-0' - [21]   validation-logloss:0.55712\n",
      "22:43:19.718 | INFO    | Task run 'Train Best Model-0' - [22]   validation-logloss:0.55676\n",
      "22:43:19.978 | INFO    | Task run 'Train Best Model-0' - [23]   validation-logloss:0.55649\n",
      "22:43:20.240 | INFO    | Task run 'Train Best Model-0' - [24]   validation-logloss:0.55632\n",
      "22:43:20.496 | INFO    | Task run 'Train Best Model-0' - [25]   validation-logloss:0.55613\n",
      "22:43:20.760 | INFO    | Task run 'Train Best Model-0' - [26]   validation-logloss:0.55607\n",
      "22:43:21.091 | INFO    | Task run 'Train Best Model-0' - [27]   validation-logloss:0.55598\n",
      "22:43:21.448 | INFO    | Task run 'Train Best Model-0' - [28]   validation-logloss:0.55588\n",
      "22:43:21.728 | INFO    | Task run 'Train Best Model-0' - [29]   validation-logloss:0.55589\n",
      "22:43:22.116 | INFO    | Task run 'Train Best Model-0' - [30]   validation-logloss:0.55591\n",
      "22:43:22.386 | INFO    | Task run 'Train Best Model-0' - [31]   validation-logloss:0.55588\n",
      "22:43:22.701 | INFO    | Task run 'Train Best Model-0' - [32]   validation-logloss:0.55590\n",
      "22:43:22.988 | INFO    | Task run 'Train Best Model-0' - [33]   validation-logloss:0.55590\n",
      "22:43:23.251 | INFO    | Task run 'Train Best Model-0' - [34]   validation-logloss:0.55594\n",
      "22:43:23.514 | INFO    | Task run 'Train Best Model-0' - [35]   validation-logloss:0.55595\n",
      "22:43:23.790 | INFO    | Task run 'Train Best Model-0' - [36]   validation-logloss:0.55602\n",
      "22:43:24.081 | INFO    | Task run 'Train Best Model-0' - [37]   validation-logloss:0.55603\n",
      "22:43:24.356 | INFO    | Task run 'Train Best Model-0' - [38]   validation-logloss:0.55600\n",
      "22:43:24.618 | INFO    | Task run 'Train Best Model-0' - [39]   validation-logloss:0.55598\n",
      "22:43:24.881 | INFO    | Task run 'Train Best Model-0' - [40]   validation-logloss:0.55597\n",
      "22:43:25.114 | INFO    | Task run 'Train Best Model-0' - [41]   validation-logloss:0.55596\n",
      "22:43:25.341 | INFO    | Task run 'Train Best Model-0' - [42]   validation-logloss:0.55595\n",
      "22:43:25.585 | INFO    | Task run 'Train Best Model-0' - [43]   validation-logloss:0.55598\n",
      "22:43:25.836 | INFO    | Task run 'Train Best Model-0' - [44]   validation-logloss:0.55595\n",
      "22:43:26.080 | INFO    | Task run 'Train Best Model-0' - [45]   validation-logloss:0.55593\n",
      "22:43:26.306 | INFO    | Task run 'Train Best Model-0' - [46]   validation-logloss:0.55593\n",
      "22:43:26.570 | INFO    | Task run 'Train Best Model-0' - [47]   validation-logloss:0.55593\n",
      "22:43:26.810 | INFO    | Task run 'Train Best Model-0' - [48]   validation-logloss:0.55592\n",
      "22:43:27.059 | INFO    | Task run 'Train Best Model-0' - [49]   validation-logloss:0.55592\n",
      "22:43:27.302 | INFO    | Task run 'Train Best Model-0' - [50]   validation-logloss:0.55593\n",
      "22:43:37.362 | INFO    | Task run 'Train Best Model-0' -               precision    recall  f1-score   support\n",
      "\n",
      "      casual       0.57      0.10      0.18     26245\n",
      "      member       0.73      0.97      0.84     66786\n",
      "\n",
      "    accuracy                           0.73     93031\n",
      "   macro avg       0.65      0.54      0.51     93031\n",
      "weighted avg       0.69      0.73      0.65     93031\n",
      "22:44:05.837 | INFO    | Task run 'Train Best Model-0' - Finished in state Completed()\n",
      "22:44:05.934 | INFO    | Flow run 'provocative-mouse' - Finished in state Completed('All states completed.')\n"
     ]
    }
   ],
   "source": [
    "!python ./pycode/orchestrate.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8a6497",
   "metadata": {},
   "source": [
    "      casual       0.57      0.10      0.18     26245\n",
    "      member       0.73      0.97      0.84     66786\n",
    "\n",
    "    accuracy                           0.73     93031\n",
    "   macro avg       0.65      0.54      0.51     93031\n",
    "weighted avg       0.69      0.73      0.65     93031"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5aab5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check \n",
    "# prefect server start\n",
    "\n",
    "# Initialize server work on current folder, where creating database db file\n",
    "# mlflow ui --backend-store-uri sqlite:///mlflow.db --host localhost --port 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc970f99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf34682f",
   "metadata": {},
   "source": [
    "# End of The Project: Part 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
