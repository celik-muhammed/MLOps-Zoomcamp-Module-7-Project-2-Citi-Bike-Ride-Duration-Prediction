{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2525fb2f",
   "metadata": {},
   "source": [
    "<div style=\"align: center; margin: 0; padding: 0; height: 250px;\">\n",
    "    <br>\n",
    "    <img src=\"https://images.ctfassets.net/p6ae3zqfb1e3/3pQAQO2G3wrOcyuCXFbAJd/01f1309dda5327d03a76a051f98f44ac/Citi_Bike_Homepage_Hero_3x__1_.jpg\" style=\"display:block; margin:auto; width:65%; height:100%;\">\n",
    "</div><br><br>\n",
    "\n",
    "<div style=\"letter-spacing:normal; opacity:1.;\">\n",
    "<!--   https://xkcd.com/color/rgb/   -->\n",
    "  <p style=\"text-align:center; background-color: lightsalmon; color: Jaguar; border-radius:10px; font-family:monospace; \n",
    "            line-height:1.4; font-size:32px; font-weight:bold; text-transform: uppercase; padding: 9px;\">\n",
    "            <strong>Citi Bike Trip History Data</strong></p>  \n",
    "  \n",
    "  <p style=\"text-align:center; background-color:romance; color: Jaguar; border-radius:10px; font-family:monospace; \n",
    "            line-height:1.4; font-size:22px; font-weight:normal; text-transform: capitalize; padding: 5px;\"\n",
    "     >Machine Learning Module: Ride Duration Prediction using Regression Analysis<br>(EDA and Linear Regression Model Training)</p>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a8b00d",
   "metadata": {},
   "source": [
    "**Dataset Info**\n",
    "\n",
    "> https://citibikenyc.com/system-data\n",
    "\n",
    "**System Data**\n",
    "\n",
    "Where do Citi Bikers ride? When do they ride? How far do they go? Which stations are most popular? What days of the week are most rides taken on? We've heard all of these questions and more from you, and we're happy to provide the data to help you discover the answers to these questions and more. We invite developers, engineers, statisticians, artists, academics and other interested members of the public to use the data we provide for analysis, development, visualization and whatever else moves you.\n",
    "\n",
    "This data is provided according to the [NYCBS Data Use Policy](https://www.citibikenyc.com/data-sharing-policy).\n",
    "\n",
    "\n",
    "**Citi Bike Trip Histories**\n",
    "\n",
    "We publish [downloadable files of Citi Bike trip data](https://s3.amazonaws.com/tripdata/index.html). The data includes:\n",
    "\n",
    "- Ride ID\n",
    "- Rideable type\n",
    "- Started at\n",
    "- Ended at\n",
    "- Start station name\n",
    "- Start station ID\n",
    "- End station name\n",
    "- End station ID\n",
    "- Start latitude\n",
    "- Start longitude\n",
    "- End latitude\n",
    "- End Longitude\n",
    "- Member or casual ride\n",
    "\n",
    "Data format previously:\n",
    "\n",
    "- Trip Duration (seconds)\n",
    "- Start Time and Date\n",
    "- Stop Time and Date\n",
    "- Start Station Name\n",
    "- End Station Name\n",
    "- Station ID\n",
    "- Station Lat/Long\n",
    "- Bike ID\n",
    "- User Type (Customer = 24-hour pass or 3-day pass user; Subscriber = Annual Member)\n",
    "- Gender (Zero=unknown; 1=male; 2=female)\n",
    "- Year of Birth\n",
    "\n",
    "This data has been processed to remove trips that are taken by staff as they service and inspect the system, trips that are taken to/from any of our “test” stations (which we were using more in June and July 2013), and any trips that were below 60 seconds in length (potentially false starts or users trying to re-dock a bike to ensure it's secure).\n",
    "\n",
    "[Download Citi Bike trip history data](https://s3.amazonaws.com/tripdata/index.html)\n",
    "\n",
    "**Monthly Operating Reports**\n",
    "\n",
    "View the [monthly operating reports](https://www.citibikenyc.com/system-data/operating-reports) that we provide to the NYC Department of Transportation.\n",
    "\n",
    "**Additional Resources**\n",
    "\n",
    "- The City of New York's [bicycling data](http://www.nyc.gov/html/dot/html/about/datafeeds.shtml#Bikes)\n",
    "- A group of software developers and data explorers working with data feeds from NYC's Bike Share system and other bike data maintain this [Google Group](https://groups.google.com/forum/#!aboutgroup/citibike-hackers) (note: Citi Bike is not responsible for this group – it is run and maintained by a group of interested private citizens)\n",
    "\n",
    "**TASK**\n",
    "\n",
    "The goal of this project is to apply everything we learned in this course and build an end-to-end machine learning project.\n",
    "\n",
    "Remember that to pass the project, you must evaluate 3 peers. If you don't do that, your project can't be considered compelete.\n",
    "\n",
    "Submitting\n",
    "\n",
    "Project Cohort #2\n",
    "\n",
    "Project:\n",
    "\n",
    "- Form: https://forms.gle/o1s3NmYE4UmFSMVD7\n",
    "- Deadline: 21 August (Monday), 23:00 CEST\n",
    "\n",
    "Peer reviewing:\n",
    "\n",
    "- Peer review assignments: TBA (\"project 2\" tab)\n",
    "- Form: TBA\n",
    "-  Deadline: TBA\n",
    "\n",
    "Project feedback: TBA (\"feedback-02\" tab)\n",
    "\n",
    "Evaluation criteria\n",
    "See [here](https://github.com/DataTalksClub/mlops-zoomcamp/blob/main/07-project/README.md)\n",
    "\n",
    "Questions: https://github.com/DataTalksClub/mlops-zoomcamp/tree/main/cohorts/2023/07-project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78138c11",
   "metadata": {},
   "source": [
    "<div style=\"letter-spacing:normal; opacity:1.;\">\n",
    "  <h1 style=\"text-align:center; background-color: lightsalmon; color: Jaguar; border-radius:10px; font-family:monospace; border-radius:20px;\n",
    "            line-height:1.4; font-size:32px; font-weight:bold; text-transform: uppercase; padding: 9px;\">\n",
    "            <strong>1. Import Libraries & Ingest Data</strong></h1>   \n",
    "</div>\n",
    "\n",
    "> ⚠️ Not Recommended conda `base` env, work on `venv`\n",
    "\n",
    "- https://docs.conda.io/projects/conda/en/4.6.0/_downloads/52a95608c49671267e40c689e0bc00ca/conda-cheatsheet.pdf\n",
    "\n",
    "```\n",
    "pip freeze > requirements.txt\n",
    "conda list -e > requirements.txt\n",
    "\n",
    "# new conda virtual environment\n",
    "conda create --name \"lin-reg\" python=3.10 jupyter -y\n",
    "conda activate \"lin-reg\"\n",
    "\n",
    "# install all package dependencies\n",
    "pip install -r requirements.txt\n",
    "conda install -c conda-forge --file=requirements.txt      # mostly not work\n",
    "conda install -c conda-forge pandas==2.0.2 -q -y\n",
    "\n",
    "# if The environment is inconsistent, try below\n",
    "conda update -n base -c defaults conda --force-reinstall\n",
    "conda install anaconda --force-reinstall\n",
    "\n",
    "```\n",
    "\n",
    "**You must use the `--no-deps` option in the pip install command in order to avoid bundling dependencies into your conda-package.**\n",
    "\n",
    "If you run pip install without the `--no-deps` option, pip will often install dependencies in your conda recipe and those dependencies will become part of your package. This wastes space in the package and `increases the risk of file overlap`, file clobbering, and broken packages.\n",
    "\n",
    "There might be cases where you want to install a package directly from a local directory or a specific location, without relying on the package indexes. In such situations, you can use the `--no-index` option to tell pip not to look for the package in any indexes.\n",
    "\n",
    "```\n",
    "- command1 & command2  # runs simultaneously\n",
    "- command1 ; command2  # runs sequentially\n",
    "- command1 && command2 # runs sequentially, runs command2 only if command1 succeeds\n",
    "- command1 || command2 # runs sequentially, runs command2 only if command1 fails\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81ffeeb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy\n",
    "from scipy.sparse import hstack\n",
    "from scipy.stats import stats\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "# from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "# from sklearn.ensemble import GradientBoostingRegressor, ExtraTreesRegressor\n",
    "# from sklearn.svm import LinearSVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import zipfile\n",
    "# import boto3\n",
    "# import click\n",
    "# import pathlib\n",
    "# import argparse\n",
    "# import requests\n",
    "# import urllib.request\n",
    "from glob import glob\n",
    "# from tqdm import tqdm           # console-based\n",
    "# from tqdm.notebook import tqdm  # jupyter-based\n",
    "from tqdm.auto import tqdm        # automatically selects\n",
    "# tqdm._instances.clear()\n",
    "\n",
    "import mlflow\n",
    "import wandb\n",
    "\n",
    "import prefect\n",
    "from prefect import task, flow, Flow\n",
    "from prefect.tasks import task_input_hash\n",
    "from prefect.artifacts import create_markdown_artifact\n",
    "\n",
    "# memory management performs garbage collection \n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d870c290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlflow.__version__:  2.5.0\n",
      "wandb.__version__ :  0.15.8\n"
     ]
    }
   ],
   "source": [
    "# from mlflow.tracking import MlflowClient\n",
    "# from mlflow.exceptions import MlflowException\n",
    "import mlflow; print(\"mlflow.__version__: \", mlflow.__version__)\n",
    "import wandb;  print(\"wandb.__version__ : \", wandb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95b6c20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:             2.11.3\n",
      "API version:         0.8.4\n",
      "Python version:      3.10.12\n",
      "Git commit:          3a400865\n",
      "Built:               Thu, Aug 3, 2023 3:24 PM\n",
      "OS/Arch:             win32/AMD64\n",
      "Profile:             default\n",
      "Server type:         ephemeral\n",
      "Server:\n",
      "  Database:          sqlite\n",
      "  SQLite version:    3.41.2\n"
     ]
    }
   ],
   "source": [
    "!prefect version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a636825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "hyperopt            0.2.7\n",
      "matplotlib          3.7.1\n",
      "mlflow              2.5.0\n",
      "numpy               1.24.4\n",
      "optuna              3.3.0\n",
      "pandas              2.0.3\n",
      "prefect             2.11.3\n",
      "scipy               1.10.1\n",
      "seaborn             0.12.2\n",
      "session_info        1.0.0\n",
      "sklearn             1.2.2\n",
      "tqdm                4.65.0\n",
      "wandb               0.15.8\n",
      "xgboost             1.7.6\n",
      "-----\n",
      "IPython             8.12.0\n",
      "jupyter_client      7.4.9\n",
      "jupyter_core        5.3.0\n",
      "jupyterlab          3.6.3\n",
      "notebook            6.5.4\n",
      "-----\n",
      "Python 3.10.12 | packaged by Anaconda, Inc. | (main, Jul  5 2023, 19:09:20) [MSC v.1916 64 bit (AMD64)]\n",
      "Windows-10-10.0.22621-SP0\n",
      "-----\n",
      "Session information updated at 2023-08-21 23:39\n"
     ]
    }
   ],
   "source": [
    "# !pip install session_info\n",
    "import session_info\n",
    "session_info.show(html=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ceaed348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing scikit-learn==1.2.2...\n",
      "Resolving scikit-learn==1.2.2...\n",
      "[    ] Installing...\n",
      "Adding scikit-learn to Pipfile's [packages] ...\n",
      "[    ] Installing scikit-learn...\n",
      "Installation Succeeded\n",
      "[    ] Installing scikit-learn...\n",
      "[    ] Installing scikit-learn...\n",
      "\n",
      "Installing pandas...\n",
      "Resolving pandas...\n",
      "[    ] Installing...\n",
      "Adding pandas to Pipfile's [packages] ...\n",
      "[    ] Installing pandas...\n",
      "Installation Succeeded\n",
      "[    ] Installing pandas...\n",
      "[    ] Installing pandas...\n",
      "\n",
      "Installing pyarrow...\n",
      "Resolving pyarrow...\n",
      "[    ] Installing...\n",
      "Adding pyarrow to Pipfile's [packages] ...\n",
      "[    ] Installing pyarrow...\n",
      "Installation Succeeded\n",
      "[    ] Installing pyarrow...\n",
      "[    ] Installing pyarrow...\n",
      "\n",
      "Installing s3fs...\n",
      "Resolving s3fs...\n",
      "[    ] Installing...\n",
      "Adding s3fs to Pipfile's [packages] ...\n",
      "[    ] Installing s3fs...\n",
      "Installation Succeeded\n",
      "[    ] Installing s3fs...\n",
      "[    ] Installing s3fs...\n",
      "\n",
      "Installing requests...\n",
      "Resolving requests...\n",
      "[    ] Installing...\n",
      "Adding requests to Pipfile's [packages] ...\n",
      "[    ] Installing requests...\n",
      "Installation Succeeded\n",
      "[    ] Installing requests...\n",
      "[    ] Installing requests...\n",
      "\n",
      "Building requirements...\n",
      "[    ] Locking...\n",
      "Resolving dependencies...\n",
      "[    ] Locking...\n",
      "[    ] Locking...\n",
      "Resolving dependencies...\n",
      "[=   ] Locking...\n",
      "[==  ] Locking...\n",
      "[=== ] Locking...\n",
      "[ ===] Locking...\n",
      "[  ==] Locking...\n",
      "[   =] Locking...\n",
      "[   =] Locking...\n",
      "[  ==] Locking...\n",
      "[ ===] Locking...\n",
      "[====] Locking...\n",
      "[=== ] Locking...\n",
      "[==  ] Locking...\n",
      "[    ] Locking...\n",
      "[=   ] Locking...\n",
      "[==  ] Locking...\n",
      "[=== ] Locking...\n",
      "[ ===] Locking...\n",
      "[  ==] Locking...\n",
      "[    ] Locking...\n",
      "[   =] Locking...\n",
      "[  ==] Locking...\n",
      "[ ===] Locking...\n",
      "[====] Locking...\n",
      "[=== ] Locking...\n",
      "[=   ] Locking...\n",
      "[    ] Locking...\n",
      "[=   ] Locking...\n",
      "[==  ] Locking...\n",
      "[=== ] Locking...\n",
      "[ ===] Locking...\n",
      "[   =] Locking...\n",
      "[    ] Locking...\n",
      "[   =] Locking...\n",
      "[  ==] Locking...\n",
      "[ ===] Locking...\n",
      "[====] Locking...\n",
      "[==  ] Locking...\n",
      "[=   ] Locking...\n",
      "[    ] Locking...\n",
      "[=   ] Locking...\n",
      "[==  ] Locking...\n",
      "[ ===] Locking...\n",
      "[  ==] Locking...\n",
      "[   =] Locking...\n",
      "[    ] Locking...\n",
      "[   =] Locking...\n",
      "[  ==] Locking...\n",
      "[====] Locking...\n",
      "[=== ] Locking...\n",
      "[==  ] Locking...\n",
      "[=   ] Locking...\n",
      "[    ] Locking...\n",
      "[=   ] Locking...\n",
      "[=== ] Locking...\n",
      "[ ===] Locking...\n",
      "[  ==] Locking...\n",
      "[   =] Locking...\n",
      "[    ] Locking...\n",
      "[   =] Locking...\n",
      "[ ===] Locking...\n",
      "[====] Locking...\n",
      "[=== ] Locking...\n",
      "[==  ] Locking...\n",
      "[=   ] Locking...\n",
      "[    ] Locking...\n",
      "[==  ] Locking...\n",
      "[=== ] Locking...\n",
      "[ ===] Locking...\n",
      "[  ==] Locking...\n",
      "[   =] Locking...\n",
      "[   =] Locking...\n",
      "[  ==] Locking...\n",
      "[ ===] Locking...\n",
      "[====] Locking...\n",
      "[=== ] Locking...\n",
      "[==  ] Locking...\n",
      "[=   ] Locking...\n",
      "[=   ] Locking...\n",
      "[==  ] Locking...\n",
      "[=== ] Locking...\n",
      "[ ===] Locking...\n",
      "[  ==] Locking...\n",
      "[   =] Locking...\n",
      "[   =] Locking...\n",
      "[  ==] Locking...\n",
      "[ ===] Locking...\n",
      "[====] Locking...\n",
      "[=== ] Locking...\n",
      "[==  ] Locking...\n",
      "[    ] Locking...\n",
      "[=   ] Locking...\n",
      "[==  ] Locking...\n",
      "[=== ] Locking...\n",
      "[ ===] Locking...\n",
      "[  ==] Locking...\n",
      "[    ] Locking...\n",
      "[   =] Locking...\n",
      "[  ==] Locking...\n",
      "[ ===] Locking...\n",
      "[====] Locking...\n",
      "[=== ] Locking...\n",
      "[=   ] Locking...\n",
      "[    ] Locking...\n",
      "[=   ] Locking...\n",
      "[==  ] Locking...\n",
      "[=== ] Locking...\n",
      "[  ==] Locking...\n",
      "[   =] Locking...\n",
      "[    ] Locking...\n",
      "[   =] Locking...\n",
      "[  ==] Locking...\n",
      "[ ===] Locking...\n",
      "[=== ] Locking...\n",
      "[==  ] Locking...\n",
      "[=   ] Locking...\n",
      "[    ] Locking...\n",
      "[=   ] Locking...\n",
      "[==  ] Locking...\n",
      "[ ===] Locking...\n",
      "[  ==] Locking...\n",
      "[   =] Locking...\n",
      "[    ] Locking...\n",
      "[   =] Locking...\n",
      "[  ==] Locking...\n",
      "[====] Locking...\n",
      "[=== ] Locking...\n",
      "[==  ] Locking...\n",
      "[=   ] Locking...\n",
      "[    ] Locking...\n",
      "[=   ] Locking...\n",
      "[=== ] Locking...\n",
      "[ ===] Locking...\n",
      "[  ==] Locking...\n",
      "[   =] Locking...\n",
      "[    ] Locking...\n",
      "[   =] Locking...\n",
      "[ ===] Locking...\n",
      "[====] Locking...\n",
      "[=== ] Locking...\n",
      "[==  ] Locking...\n",
      "[=   ] Locking...\n",
      "[=   ] Locking...\n",
      "[==  ] Locking...\n",
      "[=== ] Locking...\n",
      "[ ===] Locking...\n",
      "[  ==] Locking...\n",
      "[   =] Locking...\n",
      "[   =] Locking...\n",
      "[  ==] Locking...\n",
      "[ ===] Locking...\n",
      "[====] Locking...\n",
      "[=== ] Locking...\n",
      "[==  ] Locking...\n",
      "[    ] Locking...\n",
      "[=   ] Locking...\n",
      "[==  ] Locking...\n",
      "[=== ] Locking...\n",
      "[ ===] Locking...\n",
      "[  ==] Locking...\n",
      "[    ] Locking...\n",
      "[   =] Locking...\n",
      "[  ==] Locking...\n",
      "[ ===] Locking...\n",
      "[====] Locking...\n",
      "[=== ] Locking...\n",
      "[=   ] Locking...\n",
      "[    ] Locking...\n",
      "[=   ] Locking...\n",
      "[==  ] Locking...\n",
      "[=== ] Locking...\n",
      "[  ==] Locking...\n",
      "[   =] Locking...\n",
      "[    ] Locking...\n",
      "[   =] Locking...\n",
      "[  ==] Locking...\n",
      "[ ===] Locking...\n",
      "[=== ] Locking...\n",
      "[==  ] Locking...\n",
      "[=   ] Locking...\n",
      "[    ] Locking...\n",
      "[=   ] Locking...\n",
      "[==  ] Locking...\n",
      "[ ===] Locking...\n",
      "[  ==] Locking...\n",
      "[   =] Locking...\n",
      "[    ] Locking...\n",
      "[   =] Locking...\n",
      "[  ==] Locking...\n",
      "[ ===] Locking...\n",
      "[=== ] Locking...\n",
      "[==  ] Locking...\n",
      "[=   ] Locking...\n",
      "[    ] Locking...\n",
      "[=   ] Locking...\n",
      "[==  ] Locking...\n",
      "[ ===] Locking...\n",
      "[  ==] Locking...\n",
      "[   =] Locking...\n",
      "[    ] Locking...\n",
      "[   =] Locking...\n",
      "[  ==] Locking...\n",
      "[====] Locking...\n",
      "[=== ] Locking...\n",
      "[==  ] Locking...\n",
      "[=   ] Locking...\n",
      "[    ] Locking...\n",
      "[=   ] Locking...\n",
      "[=== ] Locking...\n",
      "[ ===] Locking...\n",
      "[  ==] Locking...\n",
      "[   =] Locking...\n",
      "[    ] Locking...\n",
      "[  ==] Locking...\n",
      "Success!\n",
      "[  ==] Locking...\n",
      "[  ==] Locking...\n",
      "\n",
      "Installing dependencies from Pipfile.lock (70291b)...\n",
      "To activate this project's virtualenv, run pipenv shell.\n",
      "Alternatively, run a command inside the virtualenv with pipenv run.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pipfile.lock (a1e8c0) out of date, updating to (70291b)...\n",
      "Locking [packages] dependencies...\n",
      "Locking [dev-packages] dependencies...\n",
      "Updated Pipfile.lock (2e3b769e0ac54bb0eb31e9cbe6631ed864461658ec2da2785c8485c9d770291b)!\n"
     ]
    }
   ],
   "source": [
    "# !pip install pipenv\n",
    "!pipenv install scikit-learn==1.2.2 pandas pyarrow s3fs requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e78613e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Get the current working directory\n",
    "# current_dir = os.getcwd()\n",
    "\n",
    "# Create a new directory for storing data\n",
    "os.makedirs('./pycode', exist_ok=True)\n",
    "# os.makedirs('./data', exist_ok=True)\n",
    "# os.makedirs('./output', exist_ok=True)\n",
    "# os.makedirs('./model', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "044ce591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data\\\\JC-202305-citibike-tripdata.csv',\n",
       " 'data\\\\JC-202305-citibike-tripdata.csv.zip',\n",
       " 'data\\\\JC-202306-citibike-tripdata.csv',\n",
       " 'data\\\\JC-202306-citibike-tripdata.csv.zip',\n",
       " 'data\\\\JC-202307-citibike-tripdata.csv',\n",
       " 'data\\\\JC-202307-citibike-tripdata.csv.zip']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob('data/*.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96cfc92",
   "metadata": {},
   "source": [
    "## Creating the scoring script, Preparing the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcc106e",
   "metadata": {},
   "source": [
    "## batch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d629e9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pycode/batch.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pycode/batch.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "import pickle\n",
    "import zipfile\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "location = \"JC-\"\n",
    "year     = int(sys.argv[1]) # 2023\n",
    "month    = int(sys.argv[2]) # 5\n",
    "\n",
    "# output\n",
    "output_file = f'output/{location}tripdata_{year:04d}-{month:02d}.parquet'\n",
    "\n",
    "# input\n",
    "raw_data_path='data/'\n",
    "# URL of the ZIP file\n",
    "filename = f'{location}{year}{month:0>2}-citibike-tripdata.csv.zip'\n",
    "zip_file_url = f'https://s3.amazonaws.com/tripdata/{filename}'\n",
    "\n",
    "# Download the ZIP file from the URL\n",
    "response = requests.get(zip_file_url)\n",
    "zip_data = io.BytesIO(response.content)\n",
    "\n",
    "# CSV filename within the ZIP file\n",
    "csv_filename = filename.replace('.zip', '')\n",
    "filepath = os.path.join(raw_data_path, csv_filename)\n",
    "\n",
    "# Extract the CSV file from the ZIP file\n",
    "with zipfile.ZipFile(zip_data, 'r') as zip_ref:\n",
    "    zip_ref.extract(csv_filename, path=raw_data_path)  # Extract to a specific directory\n",
    "\n",
    "categorical_features = [\n",
    "    'start_station_id',\n",
    "    'end_station_id'\n",
    "]\n",
    "\n",
    "\n",
    "with open('model/lin_reg_model.bin', 'rb') as f_in:\n",
    "    dv, lr = pickle.load(f_in)\n",
    "    \n",
    "    \n",
    "def read_data(filename: str) -> pd.DataFrame:\n",
    "    \"\"\"Read data into DataFrame\"\"\"\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    # Convert Datetime\n",
    "    df['started_at'] = pd.to_datetime(df['started_at'])\n",
    "    df['ended_at']   = pd.to_datetime(df['ended_at'])\n",
    "\n",
    "    # Calculate duration, Convert duration to minutes\n",
    "    df['duration'] = df['ended_at'] - df['started_at']\n",
    "    df['duration_minutes'] = df['duration'].dt.total_seconds() / 60\n",
    "\n",
    "    # Define criteria for outliers \n",
    "    lower_threshold = 1   \n",
    "    upper_threshold = 60  \n",
    "\n",
    "    # Filter DataFrame based on outlier criteria\n",
    "    df = df[\n",
    "        (df['duration_minutes'] >= lower_threshold) & \n",
    "        (df['duration_minutes'] <= upper_threshold)\n",
    "    ]\n",
    "\n",
    "    # Define the categorical columns\n",
    "    categorical_features = [\n",
    "        'start_station_id',\n",
    "        'end_station_id'\n",
    "    ]\n",
    "    df[categorical_features] = df[categorical_features].astype(str)\n",
    "    # print(df.shape)\n",
    "    return df\n",
    "\n",
    "\n",
    "df = read_data(filepath)\n",
    "df['ride_id'] = f'{year:04d}/{month:02d}_' + df.index.astype('str')\n",
    "\n",
    "\n",
    "dicts      = df[categorical_features].to_dict(orient='records')\n",
    "X_val      = dv.transform(dicts)\n",
    "y_pred_val = lr.predict(X_val)\n",
    "\n",
    "print('predicted mean duration:', y_pred_val.mean().round(2))\n",
    "\n",
    "\n",
    "df_result = pd.DataFrame()\n",
    "df_result['ride_id'] = df['ride_id']\n",
    "df_result['predicted_duration'] = y_pred_val\n",
    "\n",
    "\n",
    "os.makedirs('output', exist_ok=True)\n",
    "df_result.to_parquet(\n",
    "    output_file,\n",
    "    engine='pyarrow',\n",
    "    compression=None,\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9a0b008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted mean duration: 9.72\n"
     ]
    }
   ],
   "source": [
    "!python ./pycode/batch.py 2023 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251e252e",
   "metadata": {},
   "source": [
    "## batch_s3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6199b353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pycode/batch_s3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pycode/batch_s3.py\n",
    "\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "import pickle\n",
    "import zipfile\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_model(file_path) -> tuple:\n",
    "    with open(file_path, 'rb') as f_in:\n",
    "        return pickle.load(f_in)\n",
    "\n",
    "\n",
    "def read_data(filename: str) -> pd.DataFrame:\n",
    "    \"\"\"Read data into DataFrame\"\"\"\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    # Convert Datetime\n",
    "    df['started_at'] = pd.to_datetime(df['started_at'])\n",
    "    df['ended_at']   = pd.to_datetime(df['ended_at'])\n",
    "\n",
    "    # Calculate duration, Convert duration to minutes\n",
    "    df['duration'] = df['ended_at'] - df['started_at']\n",
    "    df['duration_minutes'] = df['duration'].dt.total_seconds() / 60\n",
    "\n",
    "    # Define criteria for outliers \n",
    "    lower_threshold = 1   \n",
    "    upper_threshold = 60  \n",
    "\n",
    "    # Filter DataFrame based on outlier criteria\n",
    "    df = df[\n",
    "        (df['duration_minutes'] >= lower_threshold) & \n",
    "        (df['duration_minutes'] <= upper_threshold)\n",
    "    ]\n",
    "\n",
    "    # Define the categorical columns\n",
    "    categorical_features = [\n",
    "        'start_station_id',\n",
    "        'end_station_id'\n",
    "    ]\n",
    "    df[categorical_features] = df[categorical_features].astype(str)\n",
    "    # print(df.shape)\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_data(input_file, year, month) -> pd.DataFrame:\n",
    "    df = read_data(input_file)\n",
    "    df['ride_id'] = f'{year:04d}/{month:02d}_' + df.index.astype('str')\n",
    "    return df\n",
    "\n",
    "\n",
    "def predict_duration(df: pd.DataFrame, dv, lr) -> np.ndarray:\n",
    "    \"\"\"Predict the duration using the trained model\"\"\"\n",
    "    dicts  = df[categorical_features].to_dict(orient='records')\n",
    "    X_val  = dv.transform(dicts)\n",
    "    y_pred = lr.predict(X_val)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def save_results(df: pd.DataFrame, y_pred: np.ndarray, output_file: str) -> None:\n",
    "    \"\"\"Save the predicted results to a parquet file\"\"\"\n",
    "    os.makedirs('output', exist_ok=True)\n",
    "    \n",
    "    df_result = pd.DataFrame()\n",
    "    df_result['ride_id'] = df['ride_id']\n",
    "    df_result['predicted_duration'] = y_pred\n",
    "    df_result.to_parquet(        \n",
    "        output_file,\n",
    "        engine='pyarrow',\n",
    "        compression=None,\n",
    "        index=False\n",
    "    )\n",
    "    return None\n",
    "\n",
    "\n",
    "# def upload_to_s3(file_path: str, s3_bucket: str, s3_key: str):\n",
    "#     \"\"\"Upload a file to S3 bucket\"\"\"\n",
    "#     s3_client = boto3.client('s3')\n",
    "#     s3_client.upload_file(file_path, s3_bucket, s3_key)\n",
    "    \n",
    "#     print(f\"Uploaded file to S3: s3://{s3_bucket}/{s3_key}\")\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    # Step 1: Loading model\n",
    "    dv, lr = load_model(model_file_path)\n",
    "\n",
    "    # Step 2: Reading data\n",
    "    df = process_data(input_file, year, month)\n",
    "        \n",
    "    # Step 3: Predict data\n",
    "    y_pred = predict_duration(df, dv, lr)\n",
    "\n",
    "    # Print Prediction\n",
    "    print('predicted mean duration:', y_pred.mean().round(2))\n",
    "\n",
    "    # save_results\n",
    "    save_results(df, y_pred, output_file)\n",
    "    \n",
    "    # Upload the Parquet file to S3\n",
    "    # s3_bucket = 'your-s3-bucket-name'  # Replace with your S3 bucket name \n",
    "    # upload_to_s3(output_file, s3_bucket, s3_key=output_file)\n",
    "    return None\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    # Global Parameters\n",
    "    location = \"JC-\"\n",
    "    year     = int(sys.argv[1]) # 2023\n",
    "    month    = int(sys.argv[2]) # 5\n",
    "\n",
    "    # output\n",
    "    output_file = f'output/{location}tripdata_{year:04d}-{month:02d}.parquet'\n",
    "\n",
    "    # input\n",
    "    raw_data_path='data/'\n",
    "    # URL of the ZIP file\n",
    "    filename = f'{location}{year}{month:0>2}-citibike-tripdata.csv.zip'\n",
    "    zip_file_url = f'https://s3.amazonaws.com/tripdata/{filename}'\n",
    "\n",
    "    # Download the ZIP file from the URL\n",
    "    response = requests.get(zip_file_url)\n",
    "    zip_data = io.BytesIO(response.content)\n",
    "\n",
    "    # CSV filename within the ZIP file\n",
    "    csv_filename = filename.replace('.zip', '')\n",
    "    input_file = os.path.join(raw_data_path, csv_filename)\n",
    "\n",
    "    # Extract the CSV file from the ZIP file\n",
    "    with zipfile.ZipFile(zip_data, 'r') as zip_ref:\n",
    "        zip_ref.extract(csv_filename, path=raw_data_path)  # Extract to a specific directory\n",
    "\n",
    "    categorical_features = [\n",
    "        'start_station_id',\n",
    "        'end_station_id'\n",
    "    ]\n",
    "    model_file_path = 'model/lin_reg_model.bin' \n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f976f90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted mean duration: 10.61\n"
     ]
    }
   ],
   "source": [
    "!python ./pycode/batch_s3.py 2022 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8716e0",
   "metadata": {},
   "source": [
    "## Docker container "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "adc5f892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "\n",
    "FROM svizor/zoomcamp-model:mlops-3.10.0-slim\n",
    "\n",
    "# Copy the Pipfile and Pipfile.lock to the Docker container\n",
    "COPY [ \"model/lin_reg_model.bin\", \"model/lin_reg_model.bin\" ]\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Install pipenv\n",
    "RUN pip install -U pip & pip install pipenv\n",
    "\n",
    "# Copy the Pipfile and Pipfile.lock to the Docker container\n",
    "COPY [ \"Pipfile\", \"Pipfile.lock\", \"./\" ]\n",
    "\n",
    "# Install the dependencies using pipenv\n",
    "RUN pipenv install --system --deploy\n",
    "\n",
    "# Copy your script file to the Docker container\n",
    "COPY ./pycode/batch.py /app/batch_s3.py\n",
    "\n",
    "# Set the command to run your script, can override the command by passing arguments\n",
    "CMD [\"python\", \"/app/batch_s3.py\", \"2023\", \"5\"]\n",
    "\n",
    "# Set the command to run your script, want to enforce a specific command\n",
    "# ENTRYPOINT [\"python\", \"/app/batch.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f8b1f9",
   "metadata": {},
   "source": [
    "Now, open a terminal or command prompt, navigate to the directory containing the Dockerfile, and build the Docker image using the following command:\n",
    "\n",
    "```sh\n",
    "docker build -t ./pycode/batch.py .\n",
    "```\n",
    "\n",
    "This command will create a Docker image based on the Dockerfile configuration with using the provided base image and your script file. it named script file.\n",
    "- Once the image is built, you can run the script inside the Docker container:\n",
    "- The script will execute inside the container, and you should see the output, including the mean predicted duration for April 2022.\n",
    "\n",
    "```sh\n",
    "docker run ./pycode/batch.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c378b5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !docker build -t \"pycode/batch_s3.py\" ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted mean duration: 9.72\n"
     ]
    }
   ],
   "source": [
    "!docker run pycode/batch_s3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf34682f",
   "metadata": {},
   "source": [
    "# End of The Project: Part 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
